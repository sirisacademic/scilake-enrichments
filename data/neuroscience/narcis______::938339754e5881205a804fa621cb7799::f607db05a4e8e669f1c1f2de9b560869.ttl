@prefix lkg-res: <http://lkg.lynx-project.eu/res/> .
@prefix eli:   <http://data.europa.eu/eli/ontology#> .
@prefix owl:   <http://www.w3.org/2002/07/owl#> .
@prefix xsd:   <http://www.w3.org/2001/XMLSchema#> .
@prefix itsrdf: <http://www.w3.org/2005/11/its/rdf#> .
@prefix lkg:   <http://lkg.lynx-project.eu/def/> .
@prefix skos:  <http://www.w3.org/2004/02/skos/core#> .
@prefix nif:   <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#> .
@prefix rdfs:  <http://www.w3.org/2000/01/rdf-schema#> .
@prefix dbo:   <http://dbpedia.org/ontology/> .
@prefix qont:  <http://qurator-projekt.de/ontology/> .
@prefix nif-ann: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#> .
@prefix dct:   <http://purl.org/dc/terms/> .
@prefix rdf:   <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dbr:   <http://dbpedia.org/resource/> .
@prefix sci-res: <http://scilake-projekt.eu/res/> .
@prefix foaf:  <http://xmlns.com/foaf/0.1/> .
@prefix scilake: <http://scilake-projekt.eu/ontology/> .

<http://scilake-project.eu/res/1cea2260#offset_42037_43482>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "BBRE holds that response enhancements cause the co-selection of features of the same object across brain regions. providing building blocks for cognitive routines 102,150 . This theoretical position is related to workspace theories of conscious perception [151][152][153][154] . Workspace theories propose that perceptual objects reach awareness if their representations are amplified and broadcasted to processors across the brain. A recent study on the neuronal correlates of conscious access examined activity elicited by low-contrast visual stimuli in the visual and frontal cortex of monkeys 155 . The stimuli were close to the threshold of perception, so that they were sometimes perceived and sometime missed. The monkeys reported the stimuli only if they reached the frontal cortex at a sufficient strength and then elicited a sudden, strong, and sustained activity. This \"ignition\" process in the frontal cortex caused a top-down enhancement of activity in the visual cortex. I therefore propose that the global neuronal workspace, in part, maps onto the neuronal mechanisms underlying object-based attention and that 'broadcasting' may correspond to the propagation of enhanced activity for the co-selection of features of the same perceptual object across brain regions. This conjecture would explain why we consciously perceive integrated object representations rather than disconnected feature sets (recently reviewed by ref. 151 )." ;
        nif:beginIndex            "42037"^^xsd:nonNegativeInteger ;
        nif:endIndex              "43482"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Incremental grouping and the global neuronal workspace for consciousness access" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/1cea2260#offset_22969_31322>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "Figure 5 illustrates the alternative hypothesis that features are bound in perception by labeling their representations across brain regions with an enhanced firing rate (BBRE) 15,16 . BBRE proposes a dual role for firing rates. Firstly, the firing rates reflect the tuning of neurons to features and base-groupings during feedforward processing 16,79 . Secondly, during incremental grouping neurons coding for to-begrouped features enhance their firing rate, above the rate of neurons representing other objects and the background (Fig. 5). At a psychological level of description, it is this 'incremental representation' that receives object-based attention 16,80 . The main prediction of BBRE is that when an object is attended, the representations of its features are enhanced across brain regions as an assembly and thereby grouped in perception. In accordance with this prediction, attention increases the firing rates of neurons in most, if not all, cortical regions. Response enhancements occur in early visual cortex 81,82 , mid-level visual areas such as V4 and MT (Figure 5C) [83][84][85][86] , inferotemporal cortex 87,88 , parietal cortex [89][90][91] and frontal cortex [92][93][94][95][96] (see Figure 5C for a few exemplary results). The typical finding is that the initial response elicited by feedforward processing is similar for attended and non-attended items and that the responses of attended objects are enhanced later, during a recurrent processing phase. Attention also increases firing rates in the subcortical structures, including the pulvinar 83,97 the LGN 98 and the basal ganglia 99 . Hence, when an object is attended, the increased neuronal responses elicited by its various features may integrate them into a coherent object representation. The co-selection of features of the same object in different brain regions implies that the distributed representations are linked by corticocortical and cortico-subcortical connections. Recent neural networks models gained insight into the neuronal mechanisms for perceptual organization by equipping biologically realistic, deep neural networks with a feedback pathway 100,101 . When an image is presented, the networks start with a feedforward sweep that detects object categories in the deeper levels of the network. They then select one of the categories in the upper layers and use the feedback pathway to enhance the relevant features at lower network levels (as in Fig. 2B,C, for one category at a time). These studies illustrate how feedback connections between cortical areas could aid in image parsing, by labeling relevant features with enhanced activity. The co-selection of features of the same object plays a central role in various cognitive tasks, such as visual search and spatial cueing 102 . For example, during visual search, the target shape is kept in working memory as a template that enhances the activity of neurons in visual cortical areas that represent the location of the target object. The enhanced activity in retinotopic areas can be read out by other brain regions, linking the targe shape to a location 103,104 . Vice versa, in spatial cuing tasks, subjects look at an array of objects, and one of them is cued. The response enhancement at the cued location spreads through the recurrent connections to also enhance the representation of the shape so that it can be identified. Attentional operations like search and cuing can be chained flexibly into longer sequences to form more complex visual routines [105][106][107] . The idea that attention causes binding has a longer history. Treisman and her co-workers 33,108 considered the role of feature binding in visual search. They noticed that visual search is slow when subjects must group features to detect the target of search, for example, when they look for a red vertical line among green vertical and red horizontal lines. Treisman's Feature Integration Theory (FIT) proposed that attention binds features into coherent object representations, for one object at a time. In a search display, attention shifts serially from one object to the next to establish the feature conjunctions, which takes time. FIT's focus was on spatial attention, binding features at a location in space. Other studies demonstrated that it is also possible to direct attention to one of two objects that overlap in space 15,17,18,109 , implying that attentional selection can also be guided by feature dimensions other than space. Furthermore, objects in images usually occupy larger regions so that a set of locations needs to be bound in perception (Figure 2A,B). For our interactions with objects, image segmentation is the process that groups the locations. In accordance with BBRE, image segmentation is associated with neuronal response enhancements in early visual brain regions, including the LGN, V1, V2, and V4 110- image elements of foreground objects elicit more activity than those that are part of the background (Figure 6E) 122,123 . Kirchberger et al. 124 recently used optogenetics in mouse visual cortex to test the causal involvement of the recurrent labeling process in V1 in figure-ground perception. The mice either detected gratings on a gray background or saw texture-defined figures on a background. When optogenetics was used to block the entire response of V1 neurons, the mice had difficulties with both types of displays, as if they were largely blind (Figure 6F). However, when the optogenetic silencing was postponed to just after the feedforward response, the mice were able to perceive gratings on a grey background, but unable to segregate figures from backgrounds. Hence, the late V1 response phase, which expresses FGM, is necessary for figure-ground perception and is read out by higher areas to plan a motor response. Because firing rates code for both feature tuning and incremental grouping, there is a need to disentangle these influences. For example, differences in contrast could interfere with incremental grouping, because objects of high contrast elicit stronger firing rates than those with lower contrast 15,125 Importantly, some neurons are influenced by grouping and others are not (cylinders and parallelopipeds in Fig. 5D). During the curve-tracing task (Figure 6A), for example, the V1 neurons that are little influenced by attention reliably represent the contrast of a curve. Other neurons are strongly modulated by attention, so that pure contrast and attentional signals can be simultaneously be decoded from the V1 population 126 . Similar results were obtained in mice performing the texturesegregation task. On average, figures elicit more activity of V1 pyramidal cells than backgrounds do, but many of them do not exhibit FGM. Some of this variability is caused by differences between the layers of V1, because the response modulation is strongest in the superficial and deep layers and weakest in input layer 4 127,128 . Intriguingly, FGM also differs between pyramidal neurons and classes of interneurons (Figure 6G) 124 . FGM is strongest for parvalbumin positive (PV) neurons. This is followed by the FGM of pyramidal cells and vasoactive intestinal peptide-expressing (VIP) neurons. The VIP neurons disinhibit the cortical column 129 and the FGM of pyramidal cells decreases if they are silenced 124 . Remarkably, FGM is reversed for somatostatin (SST) cells. Backgrounds elicit more activity of SST neurons than figures, in agreement with the hypothesis that these neurons help to suppress the representation of homogeneous image regions 130 . It seems likely that the variations in the strength of the response enhancement across neurons, layers and cell types can be exploited by neurons in other brain regions to separate the incremental grouping signal from other factors such as their feature tuning and stimulus contrast (Fig. 5D). In accordance with this view, a recent study demonstrated that the interactions between V1 and V4 differ between the initial feedforward response and the later recurrent processing phase, as if there are separate \"communication channels\" between these neurons 131 . The feedforward phase activates feature selective neurons (both populations in Fig. 5D), whereas the later, recurrent phase might correspond to the propagation of enhanced activity for incremental grouping (only cylinders in Fig. 5D)." ;
        nif:beginIndex            "22969"^^xsd:nonNegativeInteger ;
        nif:endIndex              "31322"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Object-based attention: labeling with an enhanced firing rate" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/1cea2260#offset_43483_46122>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "In this review, I discussed evidence for BBS and BBRE, mechanisms that have been proposed to create coherent object representations from features represented by assemblies of neurons across different brain regions. Existing evidence does not support BBS. Gamma synchrony is too weak to bind neurons in different brain regions and too weak to link neurons in the same area that are separated by more than a few millimetres. Synchrony at lower frequencies is variable between subjects and is not related to binding. Furthermore, optogenetic experiments demonstrated that cortical neurons are equally driven by input that is coincident and dispersed across time intervals of up to 100ms. Instead, the features of objects become bound in perception when their representations are labelled with an enhanced firing rate. This labelling process takes time and occurs for one object at a time. The assemblies of labelled neurons that form in the brain correspond to object-based attention in psychology. During action planning, the neuronal assemblies include neurons in frontal cortex representing affordances, explaining why efficient sensorimotor links form for one stimulus at a time. Many questions about the binding problem remain open. For example, it is not yet clear which connectivity patterns within and between brain regions support the spread of enhanced activity to neurons that should become part of the same assembly. There also remains much to learn about the role of pyramidal cells and interneurons in the different layers of the cortex. Furthermore, many neuroscientists have focused on the cerebral cortex, so that the contributions of interactions with subcortical structures such as the thalamus, basal ganglia, and the superior colliculus may not have been fully appreciated. Yet, I am optimistic that new insights are around the corner: the many tools of modern neuroscience, in combination with the deep learning revolution in artificial intelligence, will help us to further shape our intuitions about what the neuronal networks in our brains are capable of.  Processing starts with a feedforward pass (using a ResNet) where increasingly abstract features are extracted up to the level of object categories. This is followed by a feedback pass in which the features and pixels that belong to object classes are labelled differently. GAU stands for \"Global Attention Upsample\" and FPA for \"Feature Pyramid Attention\". D, Erroneous binding by DALL×E2. Cued with the prompt \"a blue zebra, a green tree and a red giraffe\", the network created a blue giraffe and a red zebra. Panels B and C are reproduced from Li et al. 31 ." ;
        nif:beginIndex            "43483"^^xsd:nonNegativeInteger ;
        nif:endIndex              "46122"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Conclusions" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/1cea2260#offset_31323_37268>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "An important difference between BBS and BBRE is the number of incremental groups that can coexist in perception (Figure 3B vs. Figure 5A). BBS permits several simultaneous incremental groups, labeled by unique patterns of synchrony. Because BBRE labels the incremental group with an enhanced activity level, a theoretical possibility is to define multiple firing rate levels to establish multiple, simultaneous incremental groups. However, it is difficult to conceive of mechanisms that would enable the selective routing of the information from one of these groups, say, the one with second firing rate level from the top. Instead, theories of incremental grouping proposed that there is only one incremental group that is labeled with enhanced activity and thereby segregated from the rest 16,79,107 . According to this view, features are either attended and grouped or they are not. An incremental group could, of course, be accompanied by several hardcoded feature conjunctions as base-groupings. To gain insight into the number of incremental groups, Houtkamp and Roelfsema 79 tested human participants in a curve-tracing task with multiple curves that they had to trace. Incremental grouping took place for only one curve at a time. Some intuition about the seriality of incremental grouping can be gleaned from Figure 7A, where the task is to find the curve connecting two circles (related to displays used by ref. 79 ). In a typical curve-tracing task for monkeys illustrated in Figure 7B, the animal has to make an eye movement to the larger red circle that is connected by a target curve to a small red fixation point. Recordings from area V1 revealed that the representation of the start of the curve is first enhanced before the increased activity spreads over contours farther along the curve 132 . Hence, the curve's contour elements are grouped incrementally, a process that can be based on the Gestalt grouping cues of collinearity and connectedness. In human psychophysics, the processing time also increases with the length of the to-be-traced curve 133 . Interestingly, the tracing speed depends on the distance between the target curve and the distractors 134 . The speed is highest if there is a large distance between curves. These findings can be explained if the response enhancement spreads in multiple areas, like V1, V2, and V4 107 . In higher areas neurons, have larger receptive fields and horizontal connections bridge large regions in the visual field so that tracing can make fast progress. However, if the curves are nearby, low-level areas, like V1, that represent the target curve at a higher resolution, with smaller receptive fields, may need to take over. In these lower areas the horizontal connections link neurons with nearby receptive fields, which slows down the curve-tracing speed (see ref. 132 for a discussion of the relation between the propagation speed of horizontal connections in the visual cortex and curve-tracing). In accordance with the relation between the neuronal response enhancements and object-based attention, experiments in human participants revealed that object-based attention spreads gradually over the target curve 117 . Incremental grouping also occurs when we parse natural images. The time to determine whether two cues are on the same object increases if they are farther apart or on different object parts 135 . Jeurissen et al. 136 measured processing delays while human subjects parsed line drawings (Figure 7C-F). The subjects assessed whether two cues were on the same or different objects, for line drawings of recognizable objects and modified line drawings in which the shapes could not be easily recognized (compare Figure 7C and E). The reaction time of the participants increased with the distance between the cues. Grouping was slower in narrow parts of the object and faster in wider parts, and the reaction time pattern was explained by a \"growth-cone\" incremental grouping process in which the speed of grouping scales with the size of object parts. Specifically, the reaction time increased linearly with the minimal number of circular growth cones (colored circles in Figure 7F) that can be placed in the interior of the object to connect the two cues so that growth-cone size depends on object width. The hypothesis is that an enhanced firing rate starts to spread at one of the cues within the object interior until the second cue is reached (Figure 7D), grouping labeled image regions. Larger growth cones could correspond to the receptive fields of neurons in higher areas and smaller growth cones to those in lower areas. Interestingly, the growth-cone model explained less variance for recognizable objects than for the modified ones. This difference is presumably explained by semantic segmentation based on shape recognition in the inferotemporal cortex, which might only occur for recognizable objects. Shape selective neurons could feed back to the lower areas and link features that are father apart, like the head, tail and legs of an animal (as in Figure 2B,C). Such shortcuts would explain the decrease in the accuracy of models that only evaluate local, lower-level cues for incremental grouping. Humans and monkeys make approximately three saccades per second. It is conceivable that the delays caused by image parsing, which can take up to hundreds of milliseconds, play a role in determining the typical duration of a fixation of 300ms. Every visual fixation starts with a wave of feedforward processing lasting ~100ms 19 , which is followed by a recurrent processing phase for incremental grouping, lasting 100-300ms. Studies that investigated the influence of saccades on image segmentation reported that they do not disrupt it, despite receptive field shifts in early visual areas. After every saccade, the response enhancements are routed back to the new set of neurons that now code for the contours of the attended object 137,138 ." ;
        nif:beginIndex            "31323"^^xsd:nonNegativeInteger ;
        nif:endIndex              "37268"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Perceptual grouping is time consuming and occurs for one object at a time" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/1cea2260#offset_5600_11866>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "The processing of a new image starts with a phase during which feedforward connections propagate information from lower to higher cortical areas, so that increasingly abstract features are extracted 19 . During the early phase of neuronal responses, information is propagated from lower to higher visual brain regions that encode increasingly complex features (Figure 1C). Deep neural network models 20 provide insight into how neuronal networks, including the visual system, can create meaning out of retinal inputs 21,22 . Although many deep neural network studies focus on object categorization, other feature hierarchies exist in the brain, for example, for the analysis of complex motion patterns and for the coordinate transformations that are required to control body movements during actions like navigation and grasping 7,23,24 . If the task is object recognition, feedforward processing may suffice for simpler scenes and incremental grouping may not be required 25,26 . However, there are many tasks in which it is essential to determine which of the lower and higher-level features are part of the same object. There are at least two solutions for this binding problem. The first is that groupings are hardcoded during feedforward processing. Many neurons in visual cortex code for feature conjunctions, which have been called \"base groupings\" 16 . A neuron might respond, for example, only to red vertical lines, thereby coding that these features belong together 27 . Neurons in higher brain areas coding for concepts, like zebras and giraffes, also code for feature conjunctions. A neuron that represents zebras, for example, codes for feature constellations that include a zebra head, legs, trunk, and stripes. Hence, such a neuron codes for a perceptual group, although it can be activated by many zebra pictures. However, note that the specific zebra of Figure 1A is unique, and it is unlikely that there is a neuron that responds only to this specific configuration of features. Establishing which low-level contours belong to the zebra requires a second form of perceptual grouping that is flexible and is called 'incremental grouping'. Furthermore, there is a limit to the total number of feature conjunctions that can be hardwired in the visual brain. There is a \"combinatorial explosion\" 11 as more feature conjunctions are possible than the number of neurons in the visual brain. Both BBS and BBRE propose that these additional conjunctions are formed by an incremental grouping process, which tags features of one object and segregates them from features of other objects. This incremental grouping process can occur for feature constellations that were never seen before and is thought to rely on feedback connections from higher to lower visual areas and horizontal connection between neurons in the same area (Figure 1D) 16,19 . Incremental grouping works for unfamiliar shapes, which are not represented as base-groupings and can hence only be represented in a distributed manner. For example, in Figure 2A one can see that the two parts indicated by red arrows belong to the same object, whereas the green arrow points to a different one. Unfamiliar shapes are represented by an assembly of neurons responding to features that are linked by lower-level grouping cues, such as the collinearity of image elements, their connectedness, and similarity of color and texture. These low-level grouping cues were described by Gestalt psychologists [28][29][30] and explain why we can see where one unfamiliar object ends and another one starts. Incremental grouping is flexible because one can choose to focus on any of the objects and to segregate it from the others. In the machine vision field, the problem of image segmentation has received a lot of interest in recent years. Figure 2B illustrates the output of an example neural network that performs semantic segmentation. This means that it identifies pixels belonging to different object categories 31 . The example network labeled the pixels that are part of humans with one color and the pixels of bicycles with another color. There are interesting similarities between semantic segmentation by neural networks and the perceptual grouping operations that take place in the human brain. The artificial neural network of Li et al. 31 starts with a forward pass in which the units extract increasingly abstract feature constellations in higher layers, up to semantic categories (Figure 2C). This is followed by a feedback pass that labels the pixels that belong to specific object categories. Whereas semantic segmentation gives the same label to pixels of all objects if there are multiple instances of a class, there exist related \"instance segmentation\" networks 32 . These networks give different labels to separate objects of the same category and would segregate the three cyclists in Figure 2B as different objects. The deep neural networks for image segmentation may provide crucial insights into the mechanisms for image parsing in human vision and I will point out correspondences and differences in later sections. Incremental grouping can go awry in both humans and neural networks. In humans, illusory conjunctions between features of different objects can occur if there is not enough time for recurrent processing. In a classic study, Treisman & Schmidt 33 asked participants to report two digits in a briefly presented display. A few colored letters were placed between the digits, and the subjects also reported the identity and color of the letters as a secondary task. The subjects sometimes reported a letter with the color of a different letter. For example, the subject might report a brown T, although a blue T and a brown R had been presented, suggesting that these features were correctly registered but not bound. Interestingly, such binding errors are also produced by neural networks, such OpenAI's deep network DALL×E2, which generates images from text. The network sometimes produced a red zebra and a blue giraffe when cued to produce the opposite combination of colors and shapes (Figure 2D). These illusory conjunctions illustrate the fundamental nature of the binding problem caused by the distributed and overlapping representation of multiple objects." ;
        nif:beginIndex            "5600"^^xsd:nonNegativeInteger ;
        nif:endIndex              "11866"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Hard-coded feature conjunctions and flexible codes for binding" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/1cea2260#offset_0_128>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "Solving the binding problem: assemblies form when neurons enhance their firing rate -they don't need to oscillate or synchronize" ;
        nif:beginIndex            "0"^^xsd:nonNegativeInteger ;
        nif:endIndex              "128"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Solving the binding problem: assemblies form when neurons enhance their firing rate -they don't need to oscillate or synchronize" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "title" .

<http://scilake-project.eu/res/1cea2260#offset_129_1240>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "When we look at an image, its features are represented in our visual system in a highly distributed manner, calling for a mechanism that binds them into coherent object representations. There have been different proposals for the neuronal mechanisms that can mediate binding. One hypothesis is that binding is achieved by oscillations that synchronize neurons representing features of the same perceptual object. This view allows separate communication channels between different brain areas.Another hypothesis is that binding of features that are represented in different brain regions occurs when the neurons in these areas that respond to the same object simultaneously enhance their firing rate, which would correspond to directing object-based attention to these features. This review summarizes evidence in favor and against these two hypotheses, examining the neuronal correlates of binding and assessing the time-course of perceptual grouping. I conclude that enhanced neuronal firing rates bind features into coherent object representations, whereas oscillations and synchrony are unrelated to binding." ;
        nif:beginIndex            "129"^^xsd:nonNegativeInteger ;
        nif:endIndex              "1240"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "When we look at an image, its features are represented in our visual system in a highly distributed manner, calling for a mechanism that binds them into coherent object representations. There have been different proposals for the neuronal mechanisms that can mediate binding. One hypothesis is that binding is achieved by oscillations that synchronize neurons representing features of the same perceptual object. This view allows separate communication channels between different brain areas.Another hypothesis is that binding of features that are represented in different brain regions occurs when the neurons in these areas that respond to the same object simultaneously enhance their firing rate, which would correspond to directing object-based attention to these features. This review summarizes evidence in favor and against these two hypotheses, examining the neuronal correlates of binding and assessing the time-course of perceptual grouping. I conclude that enhanced neuronal firing rates bind features into coherent object representations, whereas oscillations and synchrony are unrelated to binding." ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "abstract" .

<http://scilake-project.eu/res/1cea2260#offset_11867_20779>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "More than thirty five years ago, von der Malsburg and Schneider 34 proposed that neuronal synchronization binds features into coherent object representations. The binding-by-synchrony (BBS) hypothesis holds that neurons coding for features of the same object fire in synchrony, whereas neurons coding for features of different objects do not 12,35 . Neurons coding for features of one object (e.g. the zebra in Figure 3) would fire action potentials in synchrony and out of sync with spikes fired by neurons coding for other objects such as the giraffe (green vs. yellow spikes in Figure 3C). An important feature of BBS is that the representations of objects of interest carry their own temporal tag so that multiple incremental groups can coexist in perception (green and yellow in Figure 3B). An additional advantage of BBS is that synchrony provides a coding dimension in addition to the firing rate, enhancing the expressiveness of the representation. A study in the primary visual cortex of cats by Gray et al. 36 provided initial evidence that gamma oscillations (30-80Hz) might provide a rhythm for binding. Neurons in area V1 of cats exhibited stronger synchrony when they coded for features of a single, elongated bar of light than if they responded to two distinct and shorter bars. A related study in support of BBS recorded neuronal activity in areas 18 and PMLS of the cat and revealed that synchrony increased for neurons encoding features of the same surface compared to when they responded to different surfaces 37 . Fries et al. 38 measured synchrony in V1 in cats during binocular rivalry. In this paradigm, different stimuli are presented to the two eyes and the stimulus that is perceived alternates between the eyes. Synchrony was stronger for V1 neurons coding for the stimulus that dominated perception, suggesting a link between gamma synchrony and conscious perception. Gamma oscillations also increase when stimuli are attended [39][40][41] , although these increases are not always restricted to the gamma band 42 . It has been argued, however, that these increases in gamma may be related to the firing rate increases that accompany attention shifts, given the coupling between gamma power and neuronal firing rates 43 . The BBS hypothesis quickly gained popularity in neural network modeling studies 44,45 and the ideas were also extrapolated to functions outside vision, such as reading 46 . However, there are now several findings that seem incompatible with BBS (for earlier criticisms see refs. 47,48 ). First, gamma synchrony is a local phenomenon. Its strength quickly decays to zero for neurons separated by distances larger than a few mm [49][50][51] . Gamma synchrony can therefore not group image elements that are represented by neurons with a larger separation. For example, neurons in human V1 representing the legs and head of the giraffe in Figure 3B can be separated by centimeters, depending on the gaze position and the size of the picture on the retina. Therefore, gamma oscillations cannot be used as a code for semantic segmentation. The same holds for the binding of features across brain regions. Interregional gamma synchronization between brain regions is relatively weak 52 . To enhance sensitivity for the detection of synchrony, researchers often resort to analyzing local field potentials (i.e. the local EEG) in one or both areas 41,53,54 . Only a few studies analyzed the gamma synchronization between spikes between different areas. One of these studies by Jia et al. 55 recorded spikes in area V1 and in layer IV of area V2, which receives the feedforward input from V1. Spikes in the areas only exhibited weak synchronization if the neurons had overlapping receptive fields. The spikes in V2 were not in synchrony but lagged those in V1 by approximately 3ms, which is consistent with the feedforward propagation delay between V1 and layer IV of V2 56 . If neurons in successive stages would exhibit similar delays, they fire out of phase with the V1 neurons, which is problematic for BBS. The weak interareal gamma synchronization is problematic for setting up independent communication channels between brain regions, as is proposed by CTC 13,14 . Furthermore, the delays are prohibitive if brain regions are farther apart 13 or if they communicate through multiple routes with different numbers of intermediate stages. A study by Siegel et al. 57 may appear to provide evidence for CTC. The authors required monkeys to memorize two items during a delay. During this task, they recorded neuronal activity in the prefrontal cortex and compared decoding accuracy for the items in memory across successive phases of the gamma oscillation. Interestingly, decoding accuracy was ~7% better during the optimal phase than during the worst phase of the oscillation. The authors also observed a phase difference (corresponding to ~5 ms) between the two memory items and suggested that different phases might permit the read out of specific object information. At the same time, decoding with more than 90% of the maximum accuracy was possible for both items during all phases, which is not in line with the phase-specific signals proposed by CTC. Recent results suggests that gamma coherence between brain areas might be the result of communication rather than a prerequisite 58 . Furthermore, the gamma oscillations exhibit a dependence on the nature of the visual stimulus that is not yet fully understood. Some stimuli, such as grating patterns and red stimuli, elicit strong gamma in the EEG of humans and monkeys, whereas other stimuli do not [59][60][61][62][63] . If gamma oscillations matter for perception, one would predict that stimuli eliciting strong gamma oscillations have a special status, but such findings have, to my knowledge, not been described. Studies investigating neuronal synchrony in the visual cortex of monkeys that reported about perceptual grouping provided a direct test of BBS 51,[64][65][66] but did not support it. In an example study 51 , monkeys had to mentally trace a target curve that was connected to a fixation point to identify a larger red circle at its other end as target for an eye movement (Figure 4A). There also was a second, distracting curve that the monkeys had to ignore. The curve-tracing task requires the grouping of contour elements of the target curve and their segregation from the distractor. Recordings were made from V1 neurons with receptive fields at different positions on the curves. A first observation was that gamma synchronization did not occur. The V1 neurons did synchronize their responses, but the width of the peaks in the cross-correlation functions was ~100ms (Figure 4A), indicative of a brain rhythm with a frequency lower than gamma. Synchronization was weak, with correlation coefficients around 0.01 and, surprisingly, did not occur at all in one of the monkeys (Figure 4B). Finally, the strength of synchronization was similar, regardless of whether the V1 receptive fields fell on the same or on different curves (Figure 4A,B). Synchrony could even be dissociated from binding by creating stimuli for which grouping decreased synchrony. These results indicate that binding does not require synchronicity in area V1. Given that gamma synchrony is a local phenomenon, researchers proposed that binding could result from lower frequency alpha oscillations (8-12Hz), which synchronize neurons that are farther apart 67 . However, there are several reasons why it is unlikely that alpha oscillations play a role in binding. The first was discussed above: the strength of synchrony in V1 at lower frequencies does not reflect the grouping of contours (Figure 4A,B). Second, there is substantial variability in the strength and frequency of alpha oscillations across individuals. This variability partially reflects genetic factors, because the EEG of monozygotic twins is more similar than that of dizygotic twins (Figure 4C) 68,69 . Importantly, some humans do not have an alpha peak in the power spectrum (e.g. the upper left twin in Figure 4C, also see ref. 70 ), a finding that is presumably related to the results in one of the monkeys in Figure 4B, which did not exhibit synchrony at all. Although human participants without a clear alpha rhythm were not described further, it seems unlikely that they had binding problems, because people with binding problems have severe symptoms 71 . The absence of alpha oscillations in a fraction of the population is problematic for theories 72,73 suggesting that this rhythm is causally involved in any cognitive function. Furthermore, alpha oscillations are suppressed when visual stimuli are presented and also when subjects direct their attention to task-relevant stimuli 42,73,74 . Hence, alpha oscillations are suppressed when scene perception requires attentional grouping processes, further undermining their role in binding." ;
        nif:beginIndex            "11867"^^xsd:nonNegativeInteger ;
        nif:endIndex              "20779"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Oscillations and synchrony do not bind" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/1cea2260#offset_20780_22968>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "A cornerstone of BBS and CTC is that synchrony enhances the influence of neurons on their postsynaptic targets 75 . The efficiency with which synchronous inputs are integrated by a postsynaptic neuron depends on the distance between the membrane potential and the firing threshold of a neuron. If the membrane potential is far from the threshold, synchronous inputs help to drive a neuron. However, synchrony is less important if the membrane potential is close to the firing threshold or if the membrane potential fluctuates spontaneously 76 . Hence, whether synchrony matters for the postsynaptic drive is an empirical question and may differ between brain regions 47,77 . Histed and Maunsell 78 used optogenetics to test the influence of synchronicity on the activity of neurons in V1 of mice (Figure 4D). They compared short and strong optogenetic excitatory light pulses to weaker light pulses of longer duration. A strong light pulse of 3ms duration induced a strong synchronous burst of activity whereas weaker light pulse of 100ms caused only a small, asynchronous increase in the firing rate that was maintained for the duration of the optogenetic stimulus. Interestingly, the total number of induced action potentials did not depend on the duration of the pulse, but only on the total light energy (in mJ/mm 2 ), proportional to the total number of photons reaching the neurons (inset in Figure 4D). The ability of the mouse to detect highly synchronous bursts of V1 activity, which were induced by the short pulse, was the same as its ability to detect the same number of spikes dispersed over 100ms (Figure 4E). Although the optogenetic activation of neurons is different from how the cells are driven by visual stimuli, the result suggests that synchronicity does not increase the efficiency of post-synaptic integration, at least for intervals up to 100ms. Apparently, downstream brain regions can efficiently integrate inputs across intervals up to 100ms. In additional experiments, the authors showed that pulsing light at gamma frequencies was also inconsequential for perception, suggesting that gamma has little impact on the information transfer between brain regions." ;
        nif:beginIndex            "20780"^^xsd:nonNegativeInteger ;
        nif:endIndex              "22968"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Coincidence detection" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/1cea2260#offset_2354_5599>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "When we open our eyes, we immediately see what is there. The efficiency of our vision is a remarkable achievement of evolution. The introspective ease with which we perceive our visual surroundings masks the sophisticated machinery in our brain that supports visual perception. The image that we see is rapidly analyzed by a complex hierarchy of cortical and subcortical brain regions. Neurons in low level brain regions extract basic features such as line orientation, depth and the color of local image elements 1 . They send the information to several mid-level brain areas. For example, neurons in area MT code for motion direction 2 and neurons in area V4 code for color and shape fragments [3][4][5][6] . Neurons in mid-level areas send the information to yet higher levels for an even more abstract analysis of the visual scene 7 . Neurons at these higher levels code for the category of objects and even for the identity of specific individuals [8][9][10] . Hence, every visual object activates a complex representation that is carried by a large set of neurons across many brain regions. An important question is how the distributed and fragmented representations of objects across many areas of the visual brain can lead to a unified perception of objects against a background (Figure 1A,B). I focus here on this so-called \"binding problem\". The binding problem lures if there are multiple objects. Each of the objects activates a pattern of neurons across many brain regions and in such a representation it may not evident which features belong to one of the objects and which ones belong to the others. Which process glues the features into coherent object representations? Neuronal mechanisms for binding have been controversial. Initially researchers proposed binding-by-synchrony (BBS), which holds that the binding problem is solved by synchronous oscillations 11,12 . A related hypothesis, known as communication through coherence (CTC), holds that synchrony and oscillations permit separate channels of communication between different brain regions 13,14 . I will review evidence that casts doubt on oscillations and synchrony as the code for binding and communication. Instead, the binding problem appears to be solved by a coordination of neuronal firing rate levels across brain regions. The representations of features of an object become linked when the neurons coding for them enhance their firing rates (binding by firing rate enhancement, BBRE) 15,16 . There is a close correspondence between the firing rate enhancement and object-based attention, a concept from perceptual psychology 17,18 . Later sections will discuss how the propagation of firing rate enhancements in the visual brain corresponds to the spread of object-based attention throughout an object's representation, until all its elements are attended and thereby bound in perception. I will first outline why there is a binding problem. I will then evaluate BBS and BBRE, based on the experimental evidence in favor and against these two binding mechanisms. Toward the end of the review, I will also briefly touch upon the role of feature binding for motor programming, cognitive routines, and its relation to workspace theories of conscious perception." ;
        nif:beginIndex            "2354"^^xsd:nonNegativeInteger ;
        nif:endIndex              "5599"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Introduction" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/1cea2260>
        a                 nif:OffsetBasedString , nif:Context , scilake:ScientificDocument ;
        nif:beginIndex    "0"^^xsd:nonNegativeInteger ;
        nif:endIndex      "46670"^^xsd:nonNegativeInteger ;
        nif:isString      "Solving the binding problem: assemblies form when neurons enhance their firing rate -they don't need to oscillate or synchronize When we look at an image, its features are represented in our visual system in a highly distributed manner, calling for a mechanism that binds them into coherent object representations. There have been different proposals for the neuronal mechanisms that can mediate binding. One hypothesis is that binding is achieved by oscillations that synchronize neurons representing features of the same perceptual object. This view allows separate communication channels between different brain areas.Another hypothesis is that binding of features that are represented in different brain regions occurs when the neurons in these areas that respond to the same object simultaneously enhance their firing rate, which would correspond to directing object-based attention to these features. This review summarizes evidence in favor and against these two hypotheses, examining the neuronal correlates of binding and assessing the time-course of perceptual grouping. I conclude that enhanced neuronal firing rates bind features into coherent object representations, whereas oscillations and synchrony are unrelated to binding. When we look at an image, its features are represented in our visual system in a highly distributed manner, calling for a mechanism that binds them into coherent object representations. There have been different proposals for the neuronal mechanisms that can mediate binding. One hypothesis is that binding is achieved by oscillations that synchronize neurons representing features of the same perceptual object. This view allows separate communication channels between different brain areas. Another hypothesis is that binding of features that are represented in different brain regions occurs when the neurons in these areas that respond to the same object simultaneously enhance their firing rate, which would correspond to directing object-based attention to these features. This review summarizes evidence in favor and against these two hypotheses, examining the neuronal correlates of binding and assessing the time-course of perceptual grouping. I conclude that enhanced neuronal firing rates bind features into coherent object representations, whereas oscillations and synchrony are unrelated to binding. When we open our eyes, we immediately see what is there. The efficiency of our vision is a remarkable achievement of evolution. The introspective ease with which we perceive our visual surroundings masks the sophisticated machinery in our brain that supports visual perception. The image that we see is rapidly analyzed by a complex hierarchy of cortical and subcortical brain regions. Neurons in low level brain regions extract basic features such as line orientation, depth and the color of local image elements 1 . They send the information to several mid-level brain areas. For example, neurons in area MT code for motion direction 2 and neurons in area V4 code for color and shape fragments [3][4][5][6] . Neurons in mid-level areas send the information to yet higher levels for an even more abstract analysis of the visual scene 7 . Neurons at these higher levels code for the category of objects and even for the identity of specific individuals [8][9][10] . Hence, every visual object activates a complex representation that is carried by a large set of neurons across many brain regions. An important question is how the distributed and fragmented representations of objects across many areas of the visual brain can lead to a unified perception of objects against a background (Figure 1A,B). I focus here on this so-called \"binding problem\". The binding problem lures if there are multiple objects. Each of the objects activates a pattern of neurons across many brain regions and in such a representation it may not evident which features belong to one of the objects and which ones belong to the others. Which process glues the features into coherent object representations? Neuronal mechanisms for binding have been controversial. Initially researchers proposed binding-by-synchrony (BBS), which holds that the binding problem is solved by synchronous oscillations 11,12 . A related hypothesis, known as communication through coherence (CTC), holds that synchrony and oscillations permit separate channels of communication between different brain regions 13,14 . I will review evidence that casts doubt on oscillations and synchrony as the code for binding and communication. Instead, the binding problem appears to be solved by a coordination of neuronal firing rate levels across brain regions. The representations of features of an object become linked when the neurons coding for them enhance their firing rates (binding by firing rate enhancement, BBRE) 15,16 . There is a close correspondence between the firing rate enhancement and object-based attention, a concept from perceptual psychology 17,18 . Later sections will discuss how the propagation of firing rate enhancements in the visual brain corresponds to the spread of object-based attention throughout an object's representation, until all its elements are attended and thereby bound in perception. I will first outline why there is a binding problem. I will then evaluate BBS and BBRE, based on the experimental evidence in favor and against these two binding mechanisms. Toward the end of the review, I will also briefly touch upon the role of feature binding for motor programming, cognitive routines, and its relation to workspace theories of conscious perception. The processing of a new image starts with a phase during which feedforward connections propagate information from lower to higher cortical areas, so that increasingly abstract features are extracted 19 . During the early phase of neuronal responses, information is propagated from lower to higher visual brain regions that encode increasingly complex features (Figure 1C). Deep neural network models 20 provide insight into how neuronal networks, including the visual system, can create meaning out of retinal inputs 21,22 . Although many deep neural network studies focus on object categorization, other feature hierarchies exist in the brain, for example, for the analysis of complex motion patterns and for the coordinate transformations that are required to control body movements during actions like navigation and grasping 7,23,24 . If the task is object recognition, feedforward processing may suffice for simpler scenes and incremental grouping may not be required 25,26 . However, there are many tasks in which it is essential to determine which of the lower and higher-level features are part of the same object. There are at least two solutions for this binding problem. The first is that groupings are hardcoded during feedforward processing. Many neurons in visual cortex code for feature conjunctions, which have been called \"base groupings\" 16 . A neuron might respond, for example, only to red vertical lines, thereby coding that these features belong together 27 . Neurons in higher brain areas coding for concepts, like zebras and giraffes, also code for feature conjunctions. A neuron that represents zebras, for example, codes for feature constellations that include a zebra head, legs, trunk, and stripes. Hence, such a neuron codes for a perceptual group, although it can be activated by many zebra pictures. However, note that the specific zebra of Figure 1A is unique, and it is unlikely that there is a neuron that responds only to this specific configuration of features. Establishing which low-level contours belong to the zebra requires a second form of perceptual grouping that is flexible and is called 'incremental grouping'. Furthermore, there is a limit to the total number of feature conjunctions that can be hardwired in the visual brain. There is a \"combinatorial explosion\" 11 as more feature conjunctions are possible than the number of neurons in the visual brain. Both BBS and BBRE propose that these additional conjunctions are formed by an incremental grouping process, which tags features of one object and segregates them from features of other objects. This incremental grouping process can occur for feature constellations that were never seen before and is thought to rely on feedback connections from higher to lower visual areas and horizontal connection between neurons in the same area (Figure 1D) 16,19 . Incremental grouping works for unfamiliar shapes, which are not represented as base-groupings and can hence only be represented in a distributed manner. For example, in Figure 2A one can see that the two parts indicated by red arrows belong to the same object, whereas the green arrow points to a different one. Unfamiliar shapes are represented by an assembly of neurons responding to features that are linked by lower-level grouping cues, such as the collinearity of image elements, their connectedness, and similarity of color and texture. These low-level grouping cues were described by Gestalt psychologists [28][29][30] and explain why we can see where one unfamiliar object ends and another one starts. Incremental grouping is flexible because one can choose to focus on any of the objects and to segregate it from the others. In the machine vision field, the problem of image segmentation has received a lot of interest in recent years. Figure 2B illustrates the output of an example neural network that performs semantic segmentation. This means that it identifies pixels belonging to different object categories 31 . The example network labeled the pixels that are part of humans with one color and the pixels of bicycles with another color. There are interesting similarities between semantic segmentation by neural networks and the perceptual grouping operations that take place in the human brain. The artificial neural network of Li et al. 31 starts with a forward pass in which the units extract increasingly abstract feature constellations in higher layers, up to semantic categories (Figure 2C). This is followed by a feedback pass that labels the pixels that belong to specific object categories. Whereas semantic segmentation gives the same label to pixels of all objects if there are multiple instances of a class, there exist related \"instance segmentation\" networks 32 . These networks give different labels to separate objects of the same category and would segregate the three cyclists in Figure 2B as different objects. The deep neural networks for image segmentation may provide crucial insights into the mechanisms for image parsing in human vision and I will point out correspondences and differences in later sections. Incremental grouping can go awry in both humans and neural networks. In humans, illusory conjunctions between features of different objects can occur if there is not enough time for recurrent processing. In a classic study, Treisman & Schmidt 33 asked participants to report two digits in a briefly presented display. A few colored letters were placed between the digits, and the subjects also reported the identity and color of the letters as a secondary task. The subjects sometimes reported a letter with the color of a different letter. For example, the subject might report a brown T, although a blue T and a brown R had been presented, suggesting that these features were correctly registered but not bound. Interestingly, such binding errors are also produced by neural networks, such OpenAI's deep network DALL×E2, which generates images from text. The network sometimes produced a red zebra and a blue giraffe when cued to produce the opposite combination of colors and shapes (Figure 2D). These illusory conjunctions illustrate the fundamental nature of the binding problem caused by the distributed and overlapping representation of multiple objects. More than thirty five years ago, von der Malsburg and Schneider 34 proposed that neuronal synchronization binds features into coherent object representations. The binding-by-synchrony (BBS) hypothesis holds that neurons coding for features of the same object fire in synchrony, whereas neurons coding for features of different objects do not 12,35 . Neurons coding for features of one object (e.g. the zebra in Figure 3) would fire action potentials in synchrony and out of sync with spikes fired by neurons coding for other objects such as the giraffe (green vs. yellow spikes in Figure 3C). An important feature of BBS is that the representations of objects of interest carry their own temporal tag so that multiple incremental groups can coexist in perception (green and yellow in Figure 3B). An additional advantage of BBS is that synchrony provides a coding dimension in addition to the firing rate, enhancing the expressiveness of the representation. A study in the primary visual cortex of cats by Gray et al. 36 provided initial evidence that gamma oscillations (30-80Hz) might provide a rhythm for binding. Neurons in area V1 of cats exhibited stronger synchrony when they coded for features of a single, elongated bar of light than if they responded to two distinct and shorter bars. A related study in support of BBS recorded neuronal activity in areas 18 and PMLS of the cat and revealed that synchrony increased for neurons encoding features of the same surface compared to when they responded to different surfaces 37 . Fries et al. 38 measured synchrony in V1 in cats during binocular rivalry. In this paradigm, different stimuli are presented to the two eyes and the stimulus that is perceived alternates between the eyes. Synchrony was stronger for V1 neurons coding for the stimulus that dominated perception, suggesting a link between gamma synchrony and conscious perception. Gamma oscillations also increase when stimuli are attended [39][40][41] , although these increases are not always restricted to the gamma band 42 . It has been argued, however, that these increases in gamma may be related to the firing rate increases that accompany attention shifts, given the coupling between gamma power and neuronal firing rates 43 . The BBS hypothesis quickly gained popularity in neural network modeling studies 44,45 and the ideas were also extrapolated to functions outside vision, such as reading 46 . However, there are now several findings that seem incompatible with BBS (for earlier criticisms see refs. 47,48 ). First, gamma synchrony is a local phenomenon. Its strength quickly decays to zero for neurons separated by distances larger than a few mm [49][50][51] . Gamma synchrony can therefore not group image elements that are represented by neurons with a larger separation. For example, neurons in human V1 representing the legs and head of the giraffe in Figure 3B can be separated by centimeters, depending on the gaze position and the size of the picture on the retina. Therefore, gamma oscillations cannot be used as a code for semantic segmentation. The same holds for the binding of features across brain regions. Interregional gamma synchronization between brain regions is relatively weak 52 . To enhance sensitivity for the detection of synchrony, researchers often resort to analyzing local field potentials (i.e. the local EEG) in one or both areas 41,53,54 . Only a few studies analyzed the gamma synchronization between spikes between different areas. One of these studies by Jia et al. 55 recorded spikes in area V1 and in layer IV of area V2, which receives the feedforward input from V1. Spikes in the areas only exhibited weak synchronization if the neurons had overlapping receptive fields. The spikes in V2 were not in synchrony but lagged those in V1 by approximately 3ms, which is consistent with the feedforward propagation delay between V1 and layer IV of V2 56 . If neurons in successive stages would exhibit similar delays, they fire out of phase with the V1 neurons, which is problematic for BBS. The weak interareal gamma synchronization is problematic for setting up independent communication channels between brain regions, as is proposed by CTC 13,14 . Furthermore, the delays are prohibitive if brain regions are farther apart 13 or if they communicate through multiple routes with different numbers of intermediate stages. A study by Siegel et al. 57 may appear to provide evidence for CTC. The authors required monkeys to memorize two items during a delay. During this task, they recorded neuronal activity in the prefrontal cortex and compared decoding accuracy for the items in memory across successive phases of the gamma oscillation. Interestingly, decoding accuracy was ~7% better during the optimal phase than during the worst phase of the oscillation. The authors also observed a phase difference (corresponding to ~5 ms) between the two memory items and suggested that different phases might permit the read out of specific object information. At the same time, decoding with more than 90% of the maximum accuracy was possible for both items during all phases, which is not in line with the phase-specific signals proposed by CTC. Recent results suggests that gamma coherence between brain areas might be the result of communication rather than a prerequisite 58 . Furthermore, the gamma oscillations exhibit a dependence on the nature of the visual stimulus that is not yet fully understood. Some stimuli, such as grating patterns and red stimuli, elicit strong gamma in the EEG of humans and monkeys, whereas other stimuli do not [59][60][61][62][63] . If gamma oscillations matter for perception, one would predict that stimuli eliciting strong gamma oscillations have a special status, but such findings have, to my knowledge, not been described. Studies investigating neuronal synchrony in the visual cortex of monkeys that reported about perceptual grouping provided a direct test of BBS 51,[64][65][66] but did not support it. In an example study 51 , monkeys had to mentally trace a target curve that was connected to a fixation point to identify a larger red circle at its other end as target for an eye movement (Figure 4A). There also was a second, distracting curve that the monkeys had to ignore. The curve-tracing task requires the grouping of contour elements of the target curve and their segregation from the distractor. Recordings were made from V1 neurons with receptive fields at different positions on the curves. A first observation was that gamma synchronization did not occur. The V1 neurons did synchronize their responses, but the width of the peaks in the cross-correlation functions was ~100ms (Figure 4A), indicative of a brain rhythm with a frequency lower than gamma. Synchronization was weak, with correlation coefficients around 0.01 and, surprisingly, did not occur at all in one of the monkeys (Figure 4B). Finally, the strength of synchronization was similar, regardless of whether the V1 receptive fields fell on the same or on different curves (Figure 4A,B). Synchrony could even be dissociated from binding by creating stimuli for which grouping decreased synchrony. These results indicate that binding does not require synchronicity in area V1. Given that gamma synchrony is a local phenomenon, researchers proposed that binding could result from lower frequency alpha oscillations (8-12Hz), which synchronize neurons that are farther apart 67 . However, there are several reasons why it is unlikely that alpha oscillations play a role in binding. The first was discussed above: the strength of synchrony in V1 at lower frequencies does not reflect the grouping of contours (Figure 4A,B). Second, there is substantial variability in the strength and frequency of alpha oscillations across individuals. This variability partially reflects genetic factors, because the EEG of monozygotic twins is more similar than that of dizygotic twins (Figure 4C) 68,69 . Importantly, some humans do not have an alpha peak in the power spectrum (e.g. the upper left twin in Figure 4C, also see ref. 70 ), a finding that is presumably related to the results in one of the monkeys in Figure 4B, which did not exhibit synchrony at all. Although human participants without a clear alpha rhythm were not described further, it seems unlikely that they had binding problems, because people with binding problems have severe symptoms 71 . The absence of alpha oscillations in a fraction of the population is problematic for theories 72,73 suggesting that this rhythm is causally involved in any cognitive function. Furthermore, alpha oscillations are suppressed when visual stimuli are presented and also when subjects direct their attention to task-relevant stimuli 42,73,74 . Hence, alpha oscillations are suppressed when scene perception requires attentional grouping processes, further undermining their role in binding. A cornerstone of BBS and CTC is that synchrony enhances the influence of neurons on their postsynaptic targets 75 . The efficiency with which synchronous inputs are integrated by a postsynaptic neuron depends on the distance between the membrane potential and the firing threshold of a neuron. If the membrane potential is far from the threshold, synchronous inputs help to drive a neuron. However, synchrony is less important if the membrane potential is close to the firing threshold or if the membrane potential fluctuates spontaneously 76 . Hence, whether synchrony matters for the postsynaptic drive is an empirical question and may differ between brain regions 47,77 . Histed and Maunsell 78 used optogenetics to test the influence of synchronicity on the activity of neurons in V1 of mice (Figure 4D). They compared short and strong optogenetic excitatory light pulses to weaker light pulses of longer duration. A strong light pulse of 3ms duration induced a strong synchronous burst of activity whereas weaker light pulse of 100ms caused only a small, asynchronous increase in the firing rate that was maintained for the duration of the optogenetic stimulus. Interestingly, the total number of induced action potentials did not depend on the duration of the pulse, but only on the total light energy (in mJ/mm 2 ), proportional to the total number of photons reaching the neurons (inset in Figure 4D). The ability of the mouse to detect highly synchronous bursts of V1 activity, which were induced by the short pulse, was the same as its ability to detect the same number of spikes dispersed over 100ms (Figure 4E). Although the optogenetic activation of neurons is different from how the cells are driven by visual stimuli, the result suggests that synchronicity does not increase the efficiency of post-synaptic integration, at least for intervals up to 100ms. Apparently, downstream brain regions can efficiently integrate inputs across intervals up to 100ms. In additional experiments, the authors showed that pulsing light at gamma frequencies was also inconsequential for perception, suggesting that gamma has little impact on the information transfer between brain regions. Figure 5 illustrates the alternative hypothesis that features are bound in perception by labeling their representations across brain regions with an enhanced firing rate (BBRE) 15,16 . BBRE proposes a dual role for firing rates. Firstly, the firing rates reflect the tuning of neurons to features and base-groupings during feedforward processing 16,79 . Secondly, during incremental grouping neurons coding for to-begrouped features enhance their firing rate, above the rate of neurons representing other objects and the background (Fig. 5). At a psychological level of description, it is this 'incremental representation' that receives object-based attention 16,80 . The main prediction of BBRE is that when an object is attended, the representations of its features are enhanced across brain regions as an assembly and thereby grouped in perception. In accordance with this prediction, attention increases the firing rates of neurons in most, if not all, cortical regions. Response enhancements occur in early visual cortex 81,82 , mid-level visual areas such as V4 and MT (Figure 5C) [83][84][85][86] , inferotemporal cortex 87,88 , parietal cortex [89][90][91] and frontal cortex [92][93][94][95][96] (see Figure 5C for a few exemplary results). The typical finding is that the initial response elicited by feedforward processing is similar for attended and non-attended items and that the responses of attended objects are enhanced later, during a recurrent processing phase. Attention also increases firing rates in the subcortical structures, including the pulvinar 83,97 the LGN 98 and the basal ganglia 99 . Hence, when an object is attended, the increased neuronal responses elicited by its various features may integrate them into a coherent object representation. The co-selection of features of the same object in different brain regions implies that the distributed representations are linked by corticocortical and cortico-subcortical connections. Recent neural networks models gained insight into the neuronal mechanisms for perceptual organization by equipping biologically realistic, deep neural networks with a feedback pathway 100,101 . When an image is presented, the networks start with a feedforward sweep that detects object categories in the deeper levels of the network. They then select one of the categories in the upper layers and use the feedback pathway to enhance the relevant features at lower network levels (as in Fig. 2B,C, for one category at a time). These studies illustrate how feedback connections between cortical areas could aid in image parsing, by labeling relevant features with enhanced activity. The co-selection of features of the same object plays a central role in various cognitive tasks, such as visual search and spatial cueing 102 . For example, during visual search, the target shape is kept in working memory as a template that enhances the activity of neurons in visual cortical areas that represent the location of the target object. The enhanced activity in retinotopic areas can be read out by other brain regions, linking the targe shape to a location 103,104 . Vice versa, in spatial cuing tasks, subjects look at an array of objects, and one of them is cued. The response enhancement at the cued location spreads through the recurrent connections to also enhance the representation of the shape so that it can be identified. Attentional operations like search and cuing can be chained flexibly into longer sequences to form more complex visual routines [105][106][107] . The idea that attention causes binding has a longer history. Treisman and her co-workers 33,108 considered the role of feature binding in visual search. They noticed that visual search is slow when subjects must group features to detect the target of search, for example, when they look for a red vertical line among green vertical and red horizontal lines. Treisman's Feature Integration Theory (FIT) proposed that attention binds features into coherent object representations, for one object at a time. In a search display, attention shifts serially from one object to the next to establish the feature conjunctions, which takes time. FIT's focus was on spatial attention, binding features at a location in space. Other studies demonstrated that it is also possible to direct attention to one of two objects that overlap in space 15,17,18,109 , implying that attentional selection can also be guided by feature dimensions other than space. Furthermore, objects in images usually occupy larger regions so that a set of locations needs to be bound in perception (Figure 2A,B). For our interactions with objects, image segmentation is the process that groups the locations. In accordance with BBRE, image segmentation is associated with neuronal response enhancements in early visual brain regions, including the LGN, V1, V2, and V4 110- image elements of foreground objects elicit more activity than those that are part of the background (Figure 6E) 122,123 . Kirchberger et al. 124 recently used optogenetics in mouse visual cortex to test the causal involvement of the recurrent labeling process in V1 in figure-ground perception. The mice either detected gratings on a gray background or saw texture-defined figures on a background. When optogenetics was used to block the entire response of V1 neurons, the mice had difficulties with both types of displays, as if they were largely blind (Figure 6F). However, when the optogenetic silencing was postponed to just after the feedforward response, the mice were able to perceive gratings on a grey background, but unable to segregate figures from backgrounds. Hence, the late V1 response phase, which expresses FGM, is necessary for figure-ground perception and is read out by higher areas to plan a motor response. Because firing rates code for both feature tuning and incremental grouping, there is a need to disentangle these influences. For example, differences in contrast could interfere with incremental grouping, because objects of high contrast elicit stronger firing rates than those with lower contrast 15,125 Importantly, some neurons are influenced by grouping and others are not (cylinders and parallelopipeds in Fig. 5D). During the curve-tracing task (Figure 6A), for example, the V1 neurons that are little influenced by attention reliably represent the contrast of a curve. Other neurons are strongly modulated by attention, so that pure contrast and attentional signals can be simultaneously be decoded from the V1 population 126 . Similar results were obtained in mice performing the texturesegregation task. On average, figures elicit more activity of V1 pyramidal cells than backgrounds do, but many of them do not exhibit FGM. Some of this variability is caused by differences between the layers of V1, because the response modulation is strongest in the superficial and deep layers and weakest in input layer 4 127,128 . Intriguingly, FGM also differs between pyramidal neurons and classes of interneurons (Figure 6G) 124 . FGM is strongest for parvalbumin positive (PV) neurons. This is followed by the FGM of pyramidal cells and vasoactive intestinal peptide-expressing (VIP) neurons. The VIP neurons disinhibit the cortical column 129 and the FGM of pyramidal cells decreases if they are silenced 124 . Remarkably, FGM is reversed for somatostatin (SST) cells. Backgrounds elicit more activity of SST neurons than figures, in agreement with the hypothesis that these neurons help to suppress the representation of homogeneous image regions 130 . It seems likely that the variations in the strength of the response enhancement across neurons, layers and cell types can be exploited by neurons in other brain regions to separate the incremental grouping signal from other factors such as their feature tuning and stimulus contrast (Fig. 5D). In accordance with this view, a recent study demonstrated that the interactions between V1 and V4 differ between the initial feedforward response and the later recurrent processing phase, as if there are separate \"communication channels\" between these neurons 131 . The feedforward phase activates feature selective neurons (both populations in Fig. 5D), whereas the later, recurrent phase might correspond to the propagation of enhanced activity for incremental grouping (only cylinders in Fig. 5D). An important difference between BBS and BBRE is the number of incremental groups that can coexist in perception (Figure 3B vs. Figure 5A). BBS permits several simultaneous incremental groups, labeled by unique patterns of synchrony. Because BBRE labels the incremental group with an enhanced activity level, a theoretical possibility is to define multiple firing rate levels to establish multiple, simultaneous incremental groups. However, it is difficult to conceive of mechanisms that would enable the selective routing of the information from one of these groups, say, the one with second firing rate level from the top. Instead, theories of incremental grouping proposed that there is only one incremental group that is labeled with enhanced activity and thereby segregated from the rest 16,79,107 . According to this view, features are either attended and grouped or they are not. An incremental group could, of course, be accompanied by several hardcoded feature conjunctions as base-groupings. To gain insight into the number of incremental groups, Houtkamp and Roelfsema 79 tested human participants in a curve-tracing task with multiple curves that they had to trace. Incremental grouping took place for only one curve at a time. Some intuition about the seriality of incremental grouping can be gleaned from Figure 7A, where the task is to find the curve connecting two circles (related to displays used by ref. 79 ). In a typical curve-tracing task for monkeys illustrated in Figure 7B, the animal has to make an eye movement to the larger red circle that is connected by a target curve to a small red fixation point. Recordings from area V1 revealed that the representation of the start of the curve is first enhanced before the increased activity spreads over contours farther along the curve 132 . Hence, the curve's contour elements are grouped incrementally, a process that can be based on the Gestalt grouping cues of collinearity and connectedness. In human psychophysics, the processing time also increases with the length of the to-be-traced curve 133 . Interestingly, the tracing speed depends on the distance between the target curve and the distractors 134 . The speed is highest if there is a large distance between curves. These findings can be explained if the response enhancement spreads in multiple areas, like V1, V2, and V4 107 . In higher areas neurons, have larger receptive fields and horizontal connections bridge large regions in the visual field so that tracing can make fast progress. However, if the curves are nearby, low-level areas, like V1, that represent the target curve at a higher resolution, with smaller receptive fields, may need to take over. In these lower areas the horizontal connections link neurons with nearby receptive fields, which slows down the curve-tracing speed (see ref. 132 for a discussion of the relation between the propagation speed of horizontal connections in the visual cortex and curve-tracing). In accordance with the relation between the neuronal response enhancements and object-based attention, experiments in human participants revealed that object-based attention spreads gradually over the target curve 117 . Incremental grouping also occurs when we parse natural images. The time to determine whether two cues are on the same object increases if they are farther apart or on different object parts 135 . Jeurissen et al. 136 measured processing delays while human subjects parsed line drawings (Figure 7C-F). The subjects assessed whether two cues were on the same or different objects, for line drawings of recognizable objects and modified line drawings in which the shapes could not be easily recognized (compare Figure 7C and E). The reaction time of the participants increased with the distance between the cues. Grouping was slower in narrow parts of the object and faster in wider parts, and the reaction time pattern was explained by a \"growth-cone\" incremental grouping process in which the speed of grouping scales with the size of object parts. Specifically, the reaction time increased linearly with the minimal number of circular growth cones (colored circles in Figure 7F) that can be placed in the interior of the object to connect the two cues so that growth-cone size depends on object width. The hypothesis is that an enhanced firing rate starts to spread at one of the cues within the object interior until the second cue is reached (Figure 7D), grouping labeled image regions. Larger growth cones could correspond to the receptive fields of neurons in higher areas and smaller growth cones to those in lower areas. Interestingly, the growth-cone model explained less variance for recognizable objects than for the modified ones. This difference is presumably explained by semantic segmentation based on shape recognition in the inferotemporal cortex, which might only occur for recognizable objects. Shape selective neurons could feed back to the lower areas and link features that are father apart, like the head, tail and legs of an animal (as in Figure 2B,C). Such shortcuts would explain the decrease in the accuracy of models that only evaluate local, lower-level cues for incremental grouping. Humans and monkeys make approximately three saccades per second. It is conceivable that the delays caused by image parsing, which can take up to hundreds of milliseconds, play a role in determining the typical duration of a fixation of 300ms. Every visual fixation starts with a wave of feedforward processing lasting ~100ms 19 , which is followed by a recurrent processing phase for incremental grouping, lasting 100-300ms. Studies that investigated the influence of saccades on image segmentation reported that they do not disrupt it, despite receptive field shifts in early visual areas. After every saccade, the response enhancements are routed back to the new set of neurons that now code for the contours of the attended object 137,138 . Let us now consider the role of incremental grouping in the visual guidance of action. We direct our attention to the objects that we select for action and some theoreticians argued that the distribution of visual attention is largely determined by action planning [139][140][141] . Imagine trying to grasp one of the toys in the box of Figure 8A. Your fingers should be guided to the edges of one of the cars and you should avoid the edges of other cars. Hence, grasping can rely on the response enhancements caused by incremental grouping, highlighting the shape of the relevant object in higher areas and its edges in low-level areas. Action selection in areas of the motor and frontal cortex can utilize the incremental groups, because they signal visual features that are informative for the transportation and shaping of the hand. It is a natural generalization of the incremental grouping process to also include (pre-)motor neurons coding for the action possibilities that an object provides, like whether it could be grasped, also known as affordances 24 . In line with the linkage of perception and action, attention is directed to items that become the target of an eye or hand movement. This coupling is so strong that observers are virtually unable to recognize shapes at locations other than the location that is the target of an upcoming eye movement 142,143 . Several studies measured the simultaneous enhancement of the representations of a visual object in the visual and frontal cortex 95,96,144 . In one of these studies, Pooresmaeili et al. 144 recorded neuronal activity in areas V1, V4 and the frontal-eye fields, an area involved in the generation of eye movements, during curve-tracing (Figure 8B,C). The target curve elicited more activity than the distractor in all cortical areas, illustrating co-selection in visual and frontal cortex. Once the visual cortex propagated activity along the representation of the entire target curve including the circle at its end, the frontal cortex can read out the enhanced response to plan the eye movement. Interestingly, the response enhancements in the visual and frontal cortex occurred virtually simultaneously. In a visual search task, Zhou and Desimone 95 also observed co-selection in the visual and frontal cortex, but observed that the response enhancement in the frontal eye fields preceded that in area V4 of visual cortex. A possible explanation for the delay during search is that the working memory of the target shape is maintained in the frontal cortex and fed back to the visual cortex upon presentation of the search display. In contrast, the spread of the enhanced activity during curve-tracing is guided by the shape of the curve itself, which is represented in visual cortex. Apparently, the relative timing of incremental grouping signals in different brain regions depends on the task and provides insight in the implementation of cognitive operations in the brain. The precise connectivity patterns that ensure that features of the same objects are co-selected across brain regions remain to be fully understood. Moore & Armstrong 145 obtained important insights by directly testing the linkage of the attentional selection in the visual and frontal cortex. They activated neurons in the frontal eye fields with electrical microstimulation and observed that this increased the activity of neurons in area V4 responding to the same object. Another study used microstimulation in combination with fMRI to demonstrate how frontal eye field neurons influence activity throughout visual cortex 146 . Taken together, these experiments illustrate the strong links between representations in visual and frontal cortex. The limitation of BBRE that only one incremental group can form at a time is expected to restrict the bandwidth of information transfer between the visual and frontal cortex. In contrast, BBS might permit multiple coexisting communication channels, based on different synchronization patterns. Experiments in which subjects map multiple sensory stimuli onto different motor actions reveal dual task interference, which is an inability to carry out two tasks at the same time 147 . When two stimuli are presented in rapid succession, the initial sensory processing of the two stimuli can proceed in parallel 148 . However, later processing stages of the second stimulus must wait until these stages are freed up by the first stimulus. These results imply that there is a bottleneck in mapping stimuli into actions. Feedforward processing of different sensory stimuli can occur in parallel, but the linkage of sensory and motor representations is limited to one incremental group, thereby causing the bottleneck 149 , which is in accordance with BBRE but not with BBS. BBRE holds that response enhancements cause the co-selection of features of the same object across brain regions. providing building blocks for cognitive routines 102,150 . This theoretical position is related to workspace theories of conscious perception [151][152][153][154] . Workspace theories propose that perceptual objects reach awareness if their representations are amplified and broadcasted to processors across the brain. A recent study on the neuronal correlates of conscious access examined activity elicited by low-contrast visual stimuli in the visual and frontal cortex of monkeys 155 . The stimuli were close to the threshold of perception, so that they were sometimes perceived and sometime missed. The monkeys reported the stimuli only if they reached the frontal cortex at a sufficient strength and then elicited a sudden, strong, and sustained activity. This \"ignition\" process in the frontal cortex caused a top-down enhancement of activity in the visual cortex. I therefore propose that the global neuronal workspace, in part, maps onto the neuronal mechanisms underlying object-based attention and that 'broadcasting' may correspond to the propagation of enhanced activity for the co-selection of features of the same perceptual object across brain regions. This conjecture would explain why we consciously perceive integrated object representations rather than disconnected feature sets (recently reviewed by ref. 151 ). In this review, I discussed evidence for BBS and BBRE, mechanisms that have been proposed to create coherent object representations from features represented by assemblies of neurons across different brain regions. Existing evidence does not support BBS. Gamma synchrony is too weak to bind neurons in different brain regions and too weak to link neurons in the same area that are separated by more than a few millimetres. Synchrony at lower frequencies is variable between subjects and is not related to binding. Furthermore, optogenetic experiments demonstrated that cortical neurons are equally driven by input that is coincident and dispersed across time intervals of up to 100ms. Instead, the features of objects become bound in perception when their representations are labelled with an enhanced firing rate. This labelling process takes time and occurs for one object at a time. The assemblies of labelled neurons that form in the brain correspond to object-based attention in psychology. During action planning, the neuronal assemblies include neurons in frontal cortex representing affordances, explaining why efficient sensorimotor links form for one stimulus at a time. Many questions about the binding problem remain open. For example, it is not yet clear which connectivity patterns within and between brain regions support the spread of enhanced activity to neurons that should become part of the same assembly. There also remains much to learn about the role of pyramidal cells and interneurons in the different layers of the cortex. Furthermore, many neuroscientists have focused on the cerebral cortex, so that the contributions of interactions with subcortical structures such as the thalamus, basal ganglia, and the superior colliculus may not have been fully appreciated. Yet, I am optimistic that new insights are around the corner: the many tools of modern neuroscience, in combination with the deep learning revolution in artificial intelligence, will help us to further shape our intuitions about what the neuronal networks in our brains are capable of.  Processing starts with a feedforward pass (using a ResNet) where increasingly abstract features are extracted up to the level of object categories. This is followed by a feedback pass in which the features and pixels that belong to object classes are labelled differently. GAU stands for \"Global Attention Upsample\" and FPA for \"Feature Pyramid Attention\". D, Erroneous binding by DALL×E2. Cued with the prompt \"a blue zebra, a green tree and a red giraffe\", the network created a blue giraffe and a red zebra. Panels B and C are reproduced from Li et al. 31 . I thank Paolo Papale, Matthew Self and Martin Vinck for their valuable comments on an earlier draft of the manuscript. This work was supported by NWO (Crossover Program 17619 \"INTENSE\" and \"DBI2\", a Gravitation program of the Dutch Ministry of Science, Education and Culture), Horizon Europe (ERC advanced grant 101052963 \"NUMEROUS\"; grant agreement 899287 \"NeuraViper\") and the Human Brain Project (agreement no. 945539, \"Human Brain Project SGA3\"). Roelfsema is cofounder and shareholder of a neurotechnology start-up, Phosphoenix (Netherlands)." ;
        qont:metadata     [ dct:language  "en" ] ;
        scilake:has_part  <http://scilake-project.eu/res/1cea2260#offset_2354_5599> , <http://scilake-project.eu/res/1cea2260#offset_37269_42036> , <http://scilake-project.eu/res/1cea2260#offset_20780_22968> , <http://scilake-project.eu/res/1cea2260#offset_31323_37268> , <http://scilake-project.eu/res/1cea2260#offset_42037_43482> , <http://scilake-project.eu/res/1cea2260#offset_0_128> , <http://scilake-project.eu/res/1cea2260#offset_129_1240> , <http://scilake-project.eu/res/1cea2260#offset_22969_31322> , <http://scilake-project.eu/res/1cea2260#offset_5600_11866> , <http://scilake-project.eu/res/1cea2260#offset_46123_46670> , <http://scilake-project.eu/res/1cea2260#offset_43483_46122> , <http://scilake-project.eu/res/1cea2260#offset_11867_20779> , <http://scilake-project.eu/res/1cea2260#offset_1241_2353> .

<http://scilake-project.eu/res/1cea2260#offset_1241_2353>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "When we look at an image, its features are represented in our visual system in a highly distributed manner, calling for a mechanism that binds them into coherent object representations. There have been different proposals for the neuronal mechanisms that can mediate binding. One hypothesis is that binding is achieved by oscillations that synchronize neurons representing features of the same perceptual object. This view allows separate communication channels between different brain areas. Another hypothesis is that binding of features that are represented in different brain regions occurs when the neurons in these areas that respond to the same object simultaneously enhance their firing rate, which would correspond to directing object-based attention to these features. This review summarizes evidence in favor and against these two hypotheses, examining the neuronal correlates of binding and assessing the time-course of perceptual grouping. I conclude that enhanced neuronal firing rates bind features into coherent object representations, whereas oscillations and synchrony are unrelated to binding." ;
        nif:beginIndex            "1241"^^xsd:nonNegativeInteger ;
        nif:endIndex              "2353"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "When we look at an image, its features are represented in our visual system in a highly distributed manner, calling for a mechanism that binds them into coherent object representations. There have been different proposals for the neuronal mechanisms that can mediate binding. One hypothesis is that binding is achieved by oscillations that synchronize neurons representing features of the same perceptual object. This view allows separate communication channels between different brain areas. Another hypothesis is that binding of features that are represented in different brain regions occurs when the neurons in these areas that respond to the same object simultaneously enhance their firing rate, which would correspond to directing object-based attention to these features. This review summarizes evidence in favor and against these two hypotheses, examining the neuronal correlates of binding and assessing the time-course of perceptual grouping. I conclude that enhanced neuronal firing rates bind features into coherent object representations, whereas oscillations and synchrony are unrelated to binding." ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "abstract" .

<http://scilake-project.eu/res/1cea2260#offset_37269_42036>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "Let us now consider the role of incremental grouping in the visual guidance of action. We direct our attention to the objects that we select for action and some theoreticians argued that the distribution of visual attention is largely determined by action planning [139][140][141] . Imagine trying to grasp one of the toys in the box of Figure 8A. Your fingers should be guided to the edges of one of the cars and you should avoid the edges of other cars. Hence, grasping can rely on the response enhancements caused by incremental grouping, highlighting the shape of the relevant object in higher areas and its edges in low-level areas. Action selection in areas of the motor and frontal cortex can utilize the incremental groups, because they signal visual features that are informative for the transportation and shaping of the hand. It is a natural generalization of the incremental grouping process to also include (pre-)motor neurons coding for the action possibilities that an object provides, like whether it could be grasped, also known as affordances 24 . In line with the linkage of perception and action, attention is directed to items that become the target of an eye or hand movement. This coupling is so strong that observers are virtually unable to recognize shapes at locations other than the location that is the target of an upcoming eye movement 142,143 . Several studies measured the simultaneous enhancement of the representations of a visual object in the visual and frontal cortex 95,96,144 . In one of these studies, Pooresmaeili et al. 144 recorded neuronal activity in areas V1, V4 and the frontal-eye fields, an area involved in the generation of eye movements, during curve-tracing (Figure 8B,C). The target curve elicited more activity than the distractor in all cortical areas, illustrating co-selection in visual and frontal cortex. Once the visual cortex propagated activity along the representation of the entire target curve including the circle at its end, the frontal cortex can read out the enhanced response to plan the eye movement. Interestingly, the response enhancements in the visual and frontal cortex occurred virtually simultaneously. In a visual search task, Zhou and Desimone 95 also observed co-selection in the visual and frontal cortex, but observed that the response enhancement in the frontal eye fields preceded that in area V4 of visual cortex. A possible explanation for the delay during search is that the working memory of the target shape is maintained in the frontal cortex and fed back to the visual cortex upon presentation of the search display. In contrast, the spread of the enhanced activity during curve-tracing is guided by the shape of the curve itself, which is represented in visual cortex. Apparently, the relative timing of incremental grouping signals in different brain regions depends on the task and provides insight in the implementation of cognitive operations in the brain. The precise connectivity patterns that ensure that features of the same objects are co-selected across brain regions remain to be fully understood. Moore & Armstrong 145 obtained important insights by directly testing the linkage of the attentional selection in the visual and frontal cortex. They activated neurons in the frontal eye fields with electrical microstimulation and observed that this increased the activity of neurons in area V4 responding to the same object. Another study used microstimulation in combination with fMRI to demonstrate how frontal eye field neurons influence activity throughout visual cortex 146 . Taken together, these experiments illustrate the strong links between representations in visual and frontal cortex. The limitation of BBRE that only one incremental group can form at a time is expected to restrict the bandwidth of information transfer between the visual and frontal cortex. In contrast, BBS might permit multiple coexisting communication channels, based on different synchronization patterns. Experiments in which subjects map multiple sensory stimuli onto different motor actions reveal dual task interference, which is an inability to carry out two tasks at the same time 147 . When two stimuli are presented in rapid succession, the initial sensory processing of the two stimuli can proceed in parallel 148 . However, later processing stages of the second stimulus must wait until these stages are freed up by the first stimulus. These results imply that there is a bottleneck in mapping stimuli into actions. Feedforward processing of different sensory stimuli can occur in parallel, but the linkage of sensory and motor representations is limited to one incremental group, thereby causing the bottleneck 149 , which is in accordance with BBRE but not with BBS." ;
        nif:beginIndex            "37269"^^xsd:nonNegativeInteger ;
        nif:endIndex              "42036"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Linking perception to action" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/1cea2260#offset_46123_46670>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "I thank Paolo Papale, Matthew Self and Martin Vinck for their valuable comments on an earlier draft of the manuscript. This work was supported by NWO (Crossover Program 17619 \"INTENSE\" and \"DBI2\", a Gravitation program of the Dutch Ministry of Science, Education and Culture), Horizon Europe (ERC advanced grant 101052963 \"NUMEROUS\"; grant agreement 899287 \"NeuraViper\") and the Human Brain Project (agreement no. 945539, \"Human Brain Project SGA3\"). Roelfsema is cofounder and shareholder of a neurotechnology start-up, Phosphoenix (Netherlands)." ;
        nif:beginIndex            "46123"^^xsd:nonNegativeInteger ;
        nif:endIndex              "46670"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/1cea2260> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Declaration of interests" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "acknowledgement" .
