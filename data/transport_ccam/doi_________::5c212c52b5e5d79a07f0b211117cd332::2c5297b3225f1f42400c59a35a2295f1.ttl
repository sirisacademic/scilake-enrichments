@prefix lkg-res: <http://lkg.lynx-project.eu/res/> .
@prefix eli:   <http://data.europa.eu/eli/ontology#> .
@prefix owl:   <http://www.w3.org/2002/07/owl#> .
@prefix xsd:   <http://www.w3.org/2001/XMLSchema#> .
@prefix itsrdf: <http://www.w3.org/2005/11/its/rdf#> .
@prefix lkg:   <http://lkg.lynx-project.eu/def/> .
@prefix skos:  <http://www.w3.org/2004/02/skos/core#> .
@prefix nif:   <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#> .
@prefix rdfs:  <http://www.w3.org/2000/01/rdf-schema#> .
@prefix dbo:   <http://dbpedia.org/ontology/> .
@prefix qont:  <http://qurator-projekt.de/ontology/> .
@prefix nif-ann: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#> .
@prefix dct:   <http://purl.org/dc/terms/> .
@prefix rdf:   <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix dbr:   <http://dbpedia.org/resource/> .
@prefix sci-res: <http://scilake-projekt.eu/res/> .
@prefix foaf:  <http://xmlns.com/foaf/0.1/> .
@prefix scilake: <http://scilake-projekt.eu/ontology/> .

<http://scilake-project.eu/res/7d2c44be#offset_35544_36669>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "We evaluate the proposed algorithms by conducting extensive experiments on the challenging real-world KITTI benchmark Geiger et al. [2013] with rapidly changing environments. The seven representative datasets (namely Campus, Cola Truck, Junction, Market, Pedestrian, Red Light, and Station) have been carefully selected to cover a wide range of moving  objects in terms of quantity, size, speed, shape, occlusion, etc. To test the flexibility of the algorithms in terms of camera motion, the Campus, Pedestrian and Station sequences are acquired from a static camera set-up, while the other sequences are acquired when the camera is moving. The detailed results are synthesized in Table I Col. 2-3 and Table III Col. 2-5. The performances with the state-of-the-art methods are assessed by using the Sensitivity and Specificity metrics [Fawcett, 2006], defined as follows:  For comparison with MS-based methods, the misclassification rate metric suggested by Elhamifar and Vidal [2013]; Hu et al. [2014] is adopted. All the experiments have been conducted on a machine with Intel Quad Core i7-2.7GHz, 32GB Memory using MATLAB." ;
        nif:beginIndex            "35544"^^xsd:nonNegativeInteger ;
        nif:endIndex              "36669"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "V. EXPERIMENTS" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_31605_32630>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "In practical scenarios, the flow data might be contaminated by noise or outliers. Let where e j ∈ IR 3 is the noise or outlier entry of noise free data w 0 j . Replacing Eq. ( 12) with Eq. ( 16), we have Due to the local structure persistence and temporal flow speed consistency assumptions, the sought sparse representation from the current frame is valid for the neighbor frames. Therefore, the sparse subspace clustering problem of Eq. ( 15) can be reformulated as: ) where X t and W t are the 3D points and their flow vectors at frame t, respectively. In Eq. ( 18), E w = λ 1 n j=1 e w and E x = λ 2 n j=1 e x are energy terms with weight parameters λ 1 and λ 2 to control the influence of spatial (i.e. the 3D point coordinate) and temporal (i.e. the motion flow) factors, and we simply set λ 1 = 1 and λ 2 = 1. Note that the squared radius bound w and x are constrained to be non-negative, but not predefined. Similarly, Eq. ( 18) can be solved as a semi-definite programming problem. 3 K-mean spectral clustering on G." ;
        nif:beginIndex            "31605"^^xsd:nonNegativeInteger ;
        nif:endIndex              "32630"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "A. Influence of Noise and Outliers" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_27631_29640>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "Starting with the camera ego-motion compensation, the ICPbased point cloud registration algorithms are applied to register the given n consecutive frames of point sets. Notably, robust ICP algorithms Fitzgibbon [2003]; Zhang and Singh [2016] are preferred to obtain precise camera motion estimation. According to our expertise, ICP registration on edge and plane feature points generally yields satisfactory results, similarly to Zhang and Singh [2016]. Taking the registered point sets as input, Algo. 1 is applied to discriminate the static and the dynamic points, and to estimate the motion flows of the dynamic points. For the sake of computational efficiency, the points from ground plane are detected and removed beforehand. Note that the detection of ground plane for the data acquired by a ground-vehicle is an almost solved problem [Douillard et al., 2011;Zermas et al., 2017]. In step 4, the enclosing cylinder radius is defined as r = 0.4(1 + d/D), where d is the object to camera distance and D is the camera's maximum data acquisition distance (e.g. D = 100 meters for Velodyne HDL-64 [Lidar, 2016] 3D laser scanner). In step 7, τ S is defined as 40% of the total number of neighbours within the enclosing cylinder (also known as the sum value of the 2D histogram M). τ β = 0.175 denotes that the slope of L * is 10 degree. τ E = 1.8 is empirically studied and used for all our experiments. We recall that the Radon transform calculates the volumetric integration in both angular and positional domains. Thus, its maximum response complies to the sought optimal solution of problem (10). In Fig. 5 Col. 2, the 1D histograms from dynamic scene part have shifting effects along the flow direction, as expected. Differently, these histograms tend to overlap with each other for the static scene parts. These phenomena lead to the different properties (refer to the above discussions in Section III-B) of the motion line L * of static and dynamic points. 4 Fit an enclosing cylinder Θ(x i , X, v, r)." ;
        nif:beginIndex            "27631"^^xsd:nonNegativeInteger ;
        nif:endIndex              "29640"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "D. Implementation Details and Discussions" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_20731_21413>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "We intend to identify the moving objects, i.e. moving cars and cyclists, from a sequence of 3D point clouds. Essentially, a moving object should fulfil the criteria that a certain spatial displacement occurs within a certain time period, which can be described by a set of motion flows. To analyse, we propose the 3D Flow Field Analysis model based on the local motion consistency assumptions. Refer to the optical flow estimation Horn and Schunck [1981] and the 3D scene flow estimation Vedula et al. [2005], two assumptions are made: i. the motion behaviours of the optical flows within a small neighbourhood are similar; ii. the local geometric structure does not change rapidly." ;
        nif:beginIndex            "20731"^^xsd:nonNegativeInteger ;
        nif:endIndex              "21413"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "III. FLOW FIELD ANALYSIS" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_40504_41971>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "Quantitatively, we utilize the Misclassification Rate (same as η) to compare the performances of the different algorithms, as shown in Fig. 9. In most cases, the 2D-based approaches  , respectively. The right images are outcomes of 3Dbased approaches, namely the 3D-SSC [Jiang et al., 2016], the 3D-SMR [Jiang et al., 2017b] and the proposed 3D-SFC. Red boxes highlight the undetected or incorrectly segmented motions. achieve much higher misclassification rate, while the 3Dbased algorithms obtain relatively lower misclassification rate. Furthermore, our 3D-SFC exceptionally outperforms the compared algorithms on the evaluated datasets. Qualitatively, Fig. 10 illustrates the motion segmentation results of the compared algorithms on the Market sequence. Left column images exemplify that the 2D-based motion segmentation results are quite unsatisfactory, while the right column images of the 3D-based approaches manifest much better outcomes. Particularly, in the bottom-right image, all the moving objects are detected correctly, including the black car in the middle of the scene and the walking pedestrian on the left side. The excellent performance of the proposed 3D-Fig. 11. Top image is the full scene 3D reconstruction by using Zhang and Singh [2016] of Market sequence where numerous moving objects occur. The zoom-in regions show the immense artifacts from the walking pedestrians. Bottom image is our static-map which has distinctively higher quality." ;
        nif:beginIndex            "40504"^^xsd:nonNegativeInteger ;
        nif:endIndex              "41971"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "B. Motion Segmentation Evaluation" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_1134_10529>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "Unfortunately, practical scenarios (e.g. streets or markets) are very often highly dynamic, the scene modelling and the camera localization can become very challenging tasks, mainly due to the numerous dynamic scene parts which yield artefacts and poor localization. Our previous work Jiang et al. [2016] shows that high quality scene modelling and precise camera localization can be achieved by detecting and removing the dynamic parts. We therefore introduce our method for the robust, accurate and efficient detection of dynamic objects with high quality reconstruction of the static scene. 3D reconstruction using Lidar Odometery and Mapping (LOAM) [Zhang and Singh, 2016] technique: top image shows a decent quality 3D map of a static campus environment, while the mapping result (bottom image) of the plaza is quite unsatisfactory due to the \"ghost\" artefacts (see the zoom-in area of red boxes) caused by moving objects. Given a mobile camera-lidar platform, both the foreground and the background observations are observed as moving due to the camera's ego-motion. It is natural for human beings to identify the real moving objects due to their capability of visual object segmentation and tracking, such task is especially complex for machines and often relies on strong assumptions (sizes and velocities of the moving objects, for instance). To tackle this challenge, the Background Modelling and Subtraction-based methods Jung and Sukhatme [2004]; Fig. 2. Overview of the proposed system to detect and segment the dynamic scene parts and to reconstruct the static scene (Block 5). Given a short registered 3D point cloud sequence (Block 1), for each point of the center frame, we compute a Smooth Flow Vector (SFV) representing its motion behaviour (Block 2, Section III-A). Then, an Infinite Enclosing Cylinder is determined to bound the inlier neighbourhood with similar motions (Block 3 up, Section III-B). The drift effect that corresponds to the principal motion is obtained from the analysis of the histogram of the projections of the cloud of points onto the SFV (Block 3 down, Section III). The Sparse Flow Clustering algorithm then regroups the motion flows, as detailed in Section IV. Sheikh et al. [2009]; Yun et al. [2017]; Zhou et al. [2012] are proposed by compensating the camera ego-motion and thus, the moving objects can be detected by applying the background subtraction operation. Such methods are highly relying on the accuracy of ego-motion estimation and the lighting consistency. In a more robust manner, the Objectbased Detection, Segmentation and Tracking approaches [Cho et al., 2014;Leibe et al., 2008;Menze et al., 2018;Rashed et al., 2019;Ray and Chakraborty, 2019] are object-level motion detection by using super-pixels or object models. The moving objects are discriminated by comparing their motion trajectories versus the camera ego-motion trajectory. Such approaches are usually accurate and not sensitive to noise, while they require precise object appearance modelling and accurate object trajectory estimation. Instead, the Motion Trajectory Analysis-based techniques Brox and Malik [2010]; Elhamifar and Vidal [2013]; Vidal et al. [2008] directly segment the object's feature trajectory according to their motion subspaces, which is mathematically more elegant. Nevertheless, such approaches usually prefer continuous feature tracking and is not robust to occlusions. In real-world scenarios, the situations are more complicated due to the lack of prior knowledge of the objects, such as their sizes, their appearances, their positions, their velocities, etc. The above mentioned approaches are insufficient to perform correct and accurate moving object detection and segmentation, especially in crowded or night-time environments. Facing this challenge, we propose a solely 3D point cloud-based moving object detection approach taking into account the merits of 3D lidar (e.g. large field of view, precise measurement, night vision ability, etc). Therefore, to address the problems of dynamic object detection and motion behaviour analysis, we propose a novel framework by using the 3D Flow Field Analysis, see Fig. 2. Firstly, by compensating the sensor ego-motion (e.g. by using LOAM [Zhang and Singh, 2016], LO-Net [Li et al., 2019]), there exist continuous displacements of point sets of moving objects, while the point sets of static scene parts have no displacement. Therefore, the static scene parts should overlay together while the dynamic scene parts should not. By connecting the points of moving objects according to their temporal and spatial displacement, they become a set of motion vectors. In this regard, we propose a 3D Vector Field Analysis approach which identifies the static flows and the motion flows. After compensating the camera ego-motion, for every point in the previous frame, a flow vector is established by subtracting its nearest neighbour in the current frame. The flow vector encodes the motion direction and velocity of the objects. By exploiting these properties, the flow vectors of moving objects, so-called the motion flows, can be detected and classified into their independent motions. Moreover, a 3Dbased Sparse Flow Clustering (3D-SFC) algorithm is proposed to cluster the detected motion flows. Such 3D-SFC can robustly group the motion flows by measuring their temporal motion similarity and spatial closeness. To this end, both the static scene parts and the moving rigid objects can be reconstructed independently. In brief, the proposed framework for motion discrimination and scene reconstruction consists of four main steps, namely the Smooth Flow Vector Estimation, the Motion Flow Estimation, the Sparse Flow Clustering, and the Scene Modelling. Firstly, the Smooth Flow Vector Estimation step aims to compute the motion of each 3d point between two successive frames. Fundamentally, a 3d point's motion can be expressed as a 3d Flow Vector representing its motion speed and direction, and estimated by the subtraction of the corresponding points between two consecutive frames. Roughly, these 3d point correspondences can be efficiently established by applying naive nearest neighbour search from the current frame to the next frame. Unavoidably, the estimated 3d flow vectors can easily be contaminated by the noisy observations. Therefore, under the local motion consistency assumption, the smooth flow vector is simply estimated as the locally dominant flow vector within a local neighbourhood, such as a 3D bounding box for instance. Secondly, the Motion Flow Identification step identifies the flow vectors corresponding to the moving objects by analysing the objects' temporal and spatial displacement along their motion directions. For each flow vector, an enclosing cylinder is adapted to select the most representative neighbour points which preserve a persistent geometric structure. The projections of those points onto the current flow vector are stored in a histogram because the motion flows can be identified by detecting the shifts within the concatenated histogram from all the frames, as detailed in Section III-D. Thirdly, the Sparse Flow Field Clustering step groups the detected motion flows into their motion subspaces. Our approach is based on the distinctiveness of both motion directions and spatial distributions. We seek for the sparse selfrepresentation of motion flows from their motion subspace, which forms a sparse similarity graph. The motion flows can then be separated into independent motions by applying spectral clustering on the similarity graph. Lastly, the 3D Scene Modelling step builds the photorealistic 3d models of the dynamic outdoor environments. To densely segment the dynamic scene parts, the 3D region growing approach is applied by taking the detected motion flows as seeds. To this end, the reconstruction of the static scene is achieved by registering only the static scene parts, while the rigidly moving objects, such as moving cars, are individually reconstructed from their registered dynamic parts. This article is an extended version of our previous work [Jiang et al., 2017c] and our contribution can be summarized as follows: • We propose a robust and efficient framework for the detection and the segmentation of moving objects as well as the reconstruction of the static map from highly dynamic outdoor scenes. • We present a novel algorithm for moving object detection using 3D vector flow analysis which outperforms the state-of-the-art methods.  [Limb and Murphy, 1975] who aimed to estimate the velocity of moving objects in images from television stream. From then on, MOD becomes a very popular research field over the past few decades due to the wide ranges of applications, such as video surveillance [Reilly et al., 2010], object discovery [Pont-Tuset et al., 2017], scene modeling [Heikkila and Pietikainen, 2006], etc. Considering the system setup, there are two major branches of research: the Stationary Camera-based MOD approaches as extensively reviewed in [Benezeth et al., 2010;Elhabian et al., 2008;Joshi and Thakore, 2012], and the Moving Camera-based MOD techniques being profoundly discussed by [Jiang, 2017;Yazdi and Bouwmans, 2018]. In this article, we focus on the most related research work using moving camera setups, and discuss their positive/negative aspects comparing to the proposed algorithms." ;
        nif:beginIndex            "1134"^^xsd:nonNegativeInteger ;
        nif:endIndex              "10529"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:source                "" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_45897_47704>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "We have proposed an original 3D Flow Field Analysis (3D-FFA) algorithm for 3D Moving Object Detection (3D-MOD) under the motion consistency assumption of a local neighbourhood. We further present a novel 3D Sparse Flow Clustering (3D-SFC) approach based on the self-expressiveness property of motion flow subspace as well as the spatial closeness constraint. By integrating the proposed 3D-MOD and 3D-SFC algorithms, we show that our framework is not merely robust, efficient and accurate, but also allows photo-realistic static-map and dynamic object reconstructions by using a 2D-3D moving camera system. In many aspects, both the 3D-MOD and 3D-SFC algorithms outperform the state-of-theart methods since we have compared all these techniques on comprehensive highly dynamic real-world KITTI datasets, for which they consistently exhibit better accuracy, lower misclassication and misdetection rates, and consequently yield very high quality 3D reconstructions of static-maps as well as moving objects. In addition, the proposed framework offers great potentials in 3D city scene modelling, robot navigation and many other autonomous driving applications. As for future perspectives, since the proposed 3D-FFA algorithm makes the assumption of linear motion, it may failed to detect pure rotation motions. Thus, a complementary algorithm in dealing with pure rotation motions is preferred. Moreover, it is interesting to produce higher resolution synthetic image sequence by incorporating the image inpainting techniques. Furthermore, recent advances, such as Point FlowNet3D [Behl et al., 2019;Liu et al., 2019a], achieve very interesting results in estimating 3D scene flows, which should benefit to a better performance in motion flow field analysis as long as higher computational cost is inessential." ;
        nif:beginIndex            "45897"^^xsd:nonNegativeInteger ;
        nif:endIndex              "47704"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "VI. CONCLUSION" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_22842_24944>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "Recall the second assumption that the structure of the local point sets (Θ t = Θ(x, X t , w, r), t = 1, . . . , n) is preserved within a short time period t. Thus, the measurements of a local point set Θ t moving along w from Eq. 7 are homomorphic. Therefore, the shape of distribution of projections P t = P(Θ t , w) remain unchanged over time interval [1, t]. Let H t be a k-bin 1D histogram of projections P t at time t. The motion state of the point sets can be described by the following equation: where b is one bin of the histogram, and α(t) = βt in which β corresponds to the displacement of the histogram (or projections) from t to t + 1. Eq. ( 8) implies that the histogram is replicated from t = 1, . . . , n thanks to the temporal local structure and velocity consistency. Given a sequence of histograms H t (b), t = 1 . . . , n, our task is to estimate β and b such that Eq. ( 8) is satisfied for all t. Mathematically, it can be modelled as the followed minimization problem: To efficiently solve problem (9), the n-frame 1D histograms  L(t) = βt + b, with slope β and offset b. Note that the sought line L goes through the centres of the n-frame 1D histograms. In this regards, the optimal parameters β * and b * are obtained by Thanks to the Radon transform Deans [2007], which computes the volumetric integration in different angles at different positions in a continuous manner, problem (10) can be solved efficiently and globally by applying the Radon transform on M, as illustrated in the last column of Fig. 5. Three measurements are made along the line L * to categorize the point sets as static or dynamic. Firstly, the slope β * represents the magnitude of the motion speed, β * of a static point set is very small accordingly. Further, let s t = H t (L * ), t = 1, . . . , n be the values H t (b) on the line L * , two measurements are defined: Where S and E measure the strength and distribution homogeneity, respectively. A point set is considered to be static, if β * , S and E values are below their respective thresholds. Otherwise, the point set is assumed to be dynamic." ;
        nif:beginIndex            "22842"^^xsd:nonNegativeInteger ;
        nif:endIndex              "24944"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "B. Motion Flow Discrimination" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_29859_31604>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "In this section, we introduce our 3D SFC algorithm to further analyse the object's motion behaviour. By obtaining a set of dynamic points and their corresponding motion flows, as discussed in the above Section III, the 3D SFC intends to cluster them into multiple subsets w.r.t. their motion properties, i.e. similar motion speed, alike motion direction, and small spatial distance. Our clustering process uses the information from space subset S as well as their corresponding assignment vectors Z. On the one hand, we rely on the assumption that the vectors from one cluster are self-expressive. In other words, a flow vector can be closely approximated by the linear combination of the other flow vectors from the same cluster. On the other hand, we ensure that the clustered vector fields have bounded space subset within the predefined radius. Let X = [x 1 , . . . x j , . . . x n ] and W = [w 1 , . . . w j , . . . w n ] are 3 × n matrices of the point set and the corresponding flow vectors, the self-expressive sparse representation (similar to Elhamifar and Vidal [2013]) can be written as where the sparse n × n matrix C = [c 1 , . . . c j , . . . c n ] with c jj = 0 to avoid trivial solutions, for all j = 1, . . . n. Similarly, for a predefined squared radius bound r (where the sparsity comes from), the bounded space subset is ensured by enforcing the constraint Therefore, the sparsity-constraint relaxed optimization problem for flow clustering can be written as minimize This is a convex problem, whose optimal solution can be found by using the second order cone programming Boyd and Vandenberghe [2004]. In fact, its equivalent problem as the semi-definite programming is given by minimize ) where s ij are the elements of S." ;
        nif:beginIndex            "29859"^^xsd:nonNegativeInteger ;
        nif:endIndex              "31604"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "IV. 3D SPARSE FLOW CLUSTERING" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be>
        a                 nif:OffsetBasedString , nif:Context , scilake:ScientificDocument ;
        nif:beginIndex    "0"^^xsd:nonNegativeInteger ;
        nif:endIndex      "47704"^^xsd:nonNegativeInteger ;
        nif:isString      "Moving Object Detection by 3D Flow Field Analysis HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Unfortunately, practical scenarios (e.g. streets or markets) are very often highly dynamic, the scene modelling and the camera localization can become very challenging tasks, mainly due to the numerous dynamic scene parts which yield artefacts and poor localization. Our previous work Jiang et al. [2016] shows that high quality scene modelling and precise camera localization can be achieved by detecting and removing the dynamic parts. We therefore introduce our method for the robust, accurate and efficient detection of dynamic objects with high quality reconstruction of the static scene. 3D reconstruction using Lidar Odometery and Mapping (LOAM) [Zhang and Singh, 2016] technique: top image shows a decent quality 3D map of a static campus environment, while the mapping result (bottom image) of the plaza is quite unsatisfactory due to the \"ghost\" artefacts (see the zoom-in area of red boxes) caused by moving objects. Given a mobile camera-lidar platform, both the foreground and the background observations are observed as moving due to the camera's ego-motion. It is natural for human beings to identify the real moving objects due to their capability of visual object segmentation and tracking, such task is especially complex for machines and often relies on strong assumptions (sizes and velocities of the moving objects, for instance). To tackle this challenge, the Background Modelling and Subtraction-based methods Jung and Sukhatme [2004]; Fig. 2. Overview of the proposed system to detect and segment the dynamic scene parts and to reconstruct the static scene (Block 5). Given a short registered 3D point cloud sequence (Block 1), for each point of the center frame, we compute a Smooth Flow Vector (SFV) representing its motion behaviour (Block 2, Section III-A). Then, an Infinite Enclosing Cylinder is determined to bound the inlier neighbourhood with similar motions (Block 3 up, Section III-B). The drift effect that corresponds to the principal motion is obtained from the analysis of the histogram of the projections of the cloud of points onto the SFV (Block 3 down, Section III). The Sparse Flow Clustering algorithm then regroups the motion flows, as detailed in Section IV. Sheikh et al. [2009]; Yun et al. [2017]; Zhou et al. [2012] are proposed by compensating the camera ego-motion and thus, the moving objects can be detected by applying the background subtraction operation. Such methods are highly relying on the accuracy of ego-motion estimation and the lighting consistency. In a more robust manner, the Objectbased Detection, Segmentation and Tracking approaches [Cho et al., 2014;Leibe et al., 2008;Menze et al., 2018;Rashed et al., 2019;Ray and Chakraborty, 2019] are object-level motion detection by using super-pixels or object models. The moving objects are discriminated by comparing their motion trajectories versus the camera ego-motion trajectory. Such approaches are usually accurate and not sensitive to noise, while they require precise object appearance modelling and accurate object trajectory estimation. Instead, the Motion Trajectory Analysis-based techniques Brox and Malik [2010]; Elhamifar and Vidal [2013]; Vidal et al. [2008] directly segment the object's feature trajectory according to their motion subspaces, which is mathematically more elegant. Nevertheless, such approaches usually prefer continuous feature tracking and is not robust to occlusions. In real-world scenarios, the situations are more complicated due to the lack of prior knowledge of the objects, such as their sizes, their appearances, their positions, their velocities, etc. The above mentioned approaches are insufficient to perform correct and accurate moving object detection and segmentation, especially in crowded or night-time environments. Facing this challenge, we propose a solely 3D point cloud-based moving object detection approach taking into account the merits of 3D lidar (e.g. large field of view, precise measurement, night vision ability, etc). Therefore, to address the problems of dynamic object detection and motion behaviour analysis, we propose a novel framework by using the 3D Flow Field Analysis, see Fig. 2. Firstly, by compensating the sensor ego-motion (e.g. by using LOAM [Zhang and Singh, 2016], LO-Net [Li et al., 2019]), there exist continuous displacements of point sets of moving objects, while the point sets of static scene parts have no displacement. Therefore, the static scene parts should overlay together while the dynamic scene parts should not. By connecting the points of moving objects according to their temporal and spatial displacement, they become a set of motion vectors. In this regard, we propose a 3D Vector Field Analysis approach which identifies the static flows and the motion flows. After compensating the camera ego-motion, for every point in the previous frame, a flow vector is established by subtracting its nearest neighbour in the current frame. The flow vector encodes the motion direction and velocity of the objects. By exploiting these properties, the flow vectors of moving objects, so-called the motion flows, can be detected and classified into their independent motions. Moreover, a 3Dbased Sparse Flow Clustering (3D-SFC) algorithm is proposed to cluster the detected motion flows. Such 3D-SFC can robustly group the motion flows by measuring their temporal motion similarity and spatial closeness. To this end, both the static scene parts and the moving rigid objects can be reconstructed independently. In brief, the proposed framework for motion discrimination and scene reconstruction consists of four main steps, namely the Smooth Flow Vector Estimation, the Motion Flow Estimation, the Sparse Flow Clustering, and the Scene Modelling. Firstly, the Smooth Flow Vector Estimation step aims to compute the motion of each 3d point between two successive frames. Fundamentally, a 3d point's motion can be expressed as a 3d Flow Vector representing its motion speed and direction, and estimated by the subtraction of the corresponding points between two consecutive frames. Roughly, these 3d point correspondences can be efficiently established by applying naive nearest neighbour search from the current frame to the next frame. Unavoidably, the estimated 3d flow vectors can easily be contaminated by the noisy observations. Therefore, under the local motion consistency assumption, the smooth flow vector is simply estimated as the locally dominant flow vector within a local neighbourhood, such as a 3D bounding box for instance. Secondly, the Motion Flow Identification step identifies the flow vectors corresponding to the moving objects by analysing the objects' temporal and spatial displacement along their motion directions. For each flow vector, an enclosing cylinder is adapted to select the most representative neighbour points which preserve a persistent geometric structure. The projections of those points onto the current flow vector are stored in a histogram because the motion flows can be identified by detecting the shifts within the concatenated histogram from all the frames, as detailed in Section III-D. Thirdly, the Sparse Flow Field Clustering step groups the detected motion flows into their motion subspaces. Our approach is based on the distinctiveness of both motion directions and spatial distributions. We seek for the sparse selfrepresentation of motion flows from their motion subspace, which forms a sparse similarity graph. The motion flows can then be separated into independent motions by applying spectral clustering on the similarity graph. Lastly, the 3D Scene Modelling step builds the photorealistic 3d models of the dynamic outdoor environments. To densely segment the dynamic scene parts, the 3D region growing approach is applied by taking the detected motion flows as seeds. To this end, the reconstruction of the static scene is achieved by registering only the static scene parts, while the rigidly moving objects, such as moving cars, are individually reconstructed from their registered dynamic parts. This article is an extended version of our previous work [Jiang et al., 2017c] and our contribution can be summarized as follows: • We propose a robust and efficient framework for the detection and the segmentation of moving objects as well as the reconstruction of the static map from highly dynamic outdoor scenes. • We present a novel algorithm for moving object detection using 3D vector flow analysis which outperforms the state-of-the-art methods.  [Limb and Murphy, 1975] who aimed to estimate the velocity of moving objects in images from television stream. From then on, MOD becomes a very popular research field over the past few decades due to the wide ranges of applications, such as video surveillance [Reilly et al., 2010], object discovery [Pont-Tuset et al., 2017], scene modeling [Heikkila and Pietikainen, 2006], etc. Considering the system setup, there are two major branches of research: the Stationary Camera-based MOD approaches as extensively reviewed in [Benezeth et al., 2010;Elhabian et al., 2008;Joshi and Thakore, 2012], and the Moving Camera-based MOD techniques being profoundly discussed by [Jiang, 2017;Yazdi and Bouwmans, 2018]. In this article, we focus on the most related research work using moving camera setups, and discuss their positive/negative aspects comparing to the proposed algorithms. Feature Trajectory Analysis-based Approaches: feature trajectories are the one of the most important clues of object motions. In this context, Motion Segmentation (MS) techniques, such as the Generalized Principal Component Analysis (GPCA) [Vidal et al., 2005], RANSAC-based MS [Yan and Pollefeys, 2007], and Agglomerative Subspace Clustering [Rao et al., 2010], are proposed to group the feature trajectories according to the objects' motion subspaces. The GPCA is the representative approach that offers an algebro-geometric solution to the MS problem without the knowledge of subspace number and dimension, by representing the subspaces with a set of homogeneous polynomials. As claimed by the authors, the GPCA also provides a robust initialization to iterative techniques such as K-subspaces or Expectation Maximization algorithms. However, the determination of number of clusters and their dimensions only works for noise free data in practice. Differently, [Elhamifar and Vidal, 2013] proposed the groundbreaking Sparse Subspace Clustering (2D-SSC) algorithm relying on the self-representation property of the affine motion subspace. The 2D-SSC assumes that one feature trajectory can be represented by other feature trajectories from the same motion subspace. By incorporating the sparsity constraint on the relaxed 1 optimization, the 2D-SSC offers a robust solution to MS with outliers and achieves significantly better performances. However, the computational complexity of the 2D-SSC is proportional to the cubic of the problem size, and is therefore expensive for large scale data. As inspired, [Hu et al., 2014] proposed a SMooth Representation (2D-SMR) clustering model which outperforms the existing methods in literature by enforcing the grouping effects of the motion subspaces from image feature trajectories. To overcome the perspective projection problem of the image feature trajectories, [Jiang et al., 2016] proposed a 3D databased Sparse Subspace Clustering (3D-SSC) algorithm which achieves comparative performances against its 2D counterparts without affine motion constraint. This algorithm relies on the consistency of the tracked trajectories and is therefore sensitive to lost tracking situations and partial occlusions. To improve its robustness, [Jiang et al., 2017b] proposed a 3D-SMR algorithm which jointly benefits from the 2D-SMR in scalable feature size and tracking correspondence. In a more sophisticated manner, [Keuper et al., 2018] incorporate the low-level feature trajectory and high-level object recognition cues to achieve better performance. Inherently, feature trajectory construction is sensitive to image noise and environment change, making such approaches limited to slow camera motion and temporally consistent lighting conditions. Motion Flow Analysis-based Approaches: the motion flow, which encodes the motion magnitude and direction of the image pixel, is widely used in moving object discovery by analyzing the flow field discontinuities, such as [Huang et al., 2018;Mémin and Pérez, 2002;Yokoyama and Poggio, 2005]. The piecewise-smooth flow field are segmented by using Hierarchical [Mémin and Pérez, 2002], Level-set [Mitiche and Sekkati, 2006], or Graph-cut [Wedel et al., 2009] segmentation algorithms. More recently, [Ma et al., 2019;Menze et al., 2018] intended to detect and analyze the rigidly moving objects as Object Scene Flow (OSF) using stereo vision setup. Inspired by OSF [Menze and Geiger, 2015], Kochanov et al. [Kochanov et al., 2016] proposed to detect and segment the moving objects by propagating the OSF output to construct the static-map. The OSF-based approaches usually achieve more precise results, however, they are sensitive to the environment changes and require precise object motion model estimations. 2D Lidar-based Approaches: 2D lidar sensors (or single layer laser scanner) are widely used in industrial robots [Wang et al., 2015] or ADAS applications [Ziebinski et al., 2017] for object detection and tracking. Traditional approaches [Mertz et al., 2013;Wang et al., 2007Wang et al., , 2015] ] proposed to detect and track the moving objects along side with the simultaneous localization and mapping (SLAM) framework. The moving objects are detected by discriminating their temporal and spatial displacement. However, such method is limited to 2D laser scanner with the assumption of flat ground plane and a specific height range of moving object. 3D Lidar-based Approaches: 3D lidar (or multi-layered laser scanner) are very popular in autonomous driving applications [Levinson et al., 2011] nowadays. Taking the advantage of precise 3D point clouds, [Steinhauser et al., 2008;Sualeh and Kim, 2019;Wang et al., 2012;Ye et al., 2016] proposed to detect the possible moving objects (e.g. cars or pedestrians) and track them as motion candidates. Such approaches are trivial but require precise classifiers or feature descriptors ( [Dewan et al., 2016]) for object recognition, which is usually impractical due to the sparse point cloud and object occlusions. Apart from the above geometrical analysis-based approaches, [Börcs et al., 2017;Engelcke et al., 2017] applied the deep learning techniques resulting in more precise object recognition capability. Without relying on the object knowledge, [Asvadi et al., 2015;Azim and Aycard, 2012] utilized the occupancy grid map to statically predict and track the moving objects. Nonetheless, designing the occupancy grid size and the selection of statistical model are empirically difficult. Recently, Deep Learning -based approaches [Behl et al., 2019;Fan and Yang, 2019;Liu et al., 2019a,b] presented interesting results on 3D scene flow estimation thanks to the recent advances in computational resources and largescale training dataset. In particular, [Fan and Yang, 2019] proposed a series of point-based recurrent neural networks, i.e. the PointRNN, the PointGRU, and the PointLSTM, for dynamic point cloud forecasting via flow predicting. Such approaches adopt the spatio-temporally-local correlation to aggregate the point features and their states according to the point coordinates. [Liu et al., 2019a] focus on 3D action recognition, dynamic point cloud segmentation, and scene flow estimation with multiple frames. The proposed FlowNet-based methods apply an end-to-end model for both point feature association and flow estimation. Other approaches [Cho et al., 2014;Takabe et al., 2016] intended to fuse the image and lidar observations for MOD using photometric and depth consistencies. [Rashed et al., 2019] made use of both the camera and lidar for robust MOD in low-light autonomous driving environments. Although the deep learning-based approaches show promising results, it is difficult to collect massive training data and to have heavy computational resources during the training process. To summarize, unlike the above discussed methods, the proposed algorithms neither rely on feature tracking across the frame sequence contributing to their robustness to occlusions, nor require exhaustive machine learning training process making them being handy and easy to implement. Our MOD algorithm directly detects and segments the motion flows using raw 3D point cloud sequence without texture information. Although a rough 3D point cloud registration step is required for ego-motion compensation, the traditional ICPbased registration techniques [Fitzgibbon, 2003] are sufficient. Moreover, our method is very generic in detecting moving objects in terms of size, speed and direction. where w i ∈ R 3 , be the set of flow vectors associated to X. The 3D vector field Ω defined by X and W is notated as Ω : X → W. Given a sequence of point sets from a dynamic scene, we define S = {X t , t = 1, . . . , n} as the collection of multiple observed point sets that evolve over time t. Likewise, Z = {W t , t = 1, . . . , n − 1} is the collection of flow vectors associated to S. For two 3D point sets A and B, the vector field Ω : A → W can be obtained by the element-wise subtraction between the two point sets. We define the element-wise subtraction operation A B as where x i is an element of A, and The subtraction x i −y i defines the flow vector w i . The closest point function N (x, B) is defined as In a similar manner, the nearest neighbourhood set of points centred at x within a radius r is given by (3) We also define P(S, w), the projection of set S on the flow vector w (similarly, P(x, w) for point x), such that Projection Value We refer the illustrative examples of Eq. ( 4) to Fig. 3 and Fig. 4. In Fig. 3, a set of 3D points are projected onto the given 3D vector and the projection values are statistically represented as an n-bin one dimensional histogram. Similarly, in Fig. 4, two 3D points x 1 , x 2 are projected onto w c as two blue dots p 1 , p 2 . Note that the projection of a three dimensional point to the given 3D vector axis corresponds to its foot of perpendicular to the 3D vector, but we only take into account its scalar abscissa p on the axis. The origin of the axis is a specified 3D point, e.g. the mean values of the 3D point set. Since the mathematical representation of a 3D point is similar to a 3D vector, the projection of one 3D vector to the given 3D vector can be performed in a similar manner. Furthermore, let Θ ⊂ S be the points within an infinite cylinder centred at x c , of radius r and axis w c , given by (5) In other words, Eq. ( 5) rejects the points which have pointto-axis distances larger than the cylinder radius r, see Fig. 4 as an example. Two 3D points x 1 , x 2 are projected onto the cylinder axis wc as p 1 and p 2 . And the distances from points x 1 , x 2 to p 1 , p 2 are notated as d 1 and d 2 , respectively. Since d 1 2 = xc − x 1 2 − P(x 1 , wc) 2 ≤ r 2 , x 1 ∈ Θ is considered as inside the cylinder. In contrast, x 2 / ∈ Θ is outside the cylinder. Hereafter, we define some notations for matrix operation. Let A = (a ij ) be the element-wise representation of an m × n -sized matrix. Its column-wise representation is notated as A = [a 1 , • • • a j , . . . a n ] where a j is an m-dimensional vector. A 0 means that A is a symmetric and positive semi-definite matrix. We intend to identify the moving objects, i.e. moving cars and cyclists, from a sequence of 3D point clouds. Essentially, a moving object should fulfil the criteria that a certain spatial displacement occurs within a certain time period, which can be described by a set of motion flows. To analyse, we propose the 3D Flow Field Analysis model based on the local motion consistency assumptions. Refer to the optical flow estimation Horn and Schunck [1981] and the 3D scene flow estimation Vedula et al. [2005], two assumptions are made: i. the motion behaviours of the optical flows within a small neighbourhood are similar; ii. the local geometric structure does not change rapidly. Let a collection of n-consecutive point sets be S = {X t , t = 1, . . . , n}. For t = 1, . . . , n − 1, we compute the point-wise flow sets Z = {W t , t = 1, . . . , n − 1} which represent the motion of points over time t, as follow: Recall the definition of Eq. ( 1) and Eq. ( 2), the element-wise subtraction is performed to estimate the motion flows between frame t and frame t + 1. Due to the noisy observation of point set and the incorrect point-pair association, see Fig. 2 Block 2, the estimated motion flow using Eq. ( 6) can be incorrect. Therefore, by taking the locally homogeneous assumption of neighbouring flow vectors, we perform the smoothing of vector field by updating each w i ∈ W t as where v * i is the desired smooth flow vector to replace w i . Refer to Eq. ( 2), N = N (x i , X t , r) is the neighbourhood (within the radius r) that defines the local flow field Ω(N ). Actually, Eq. ( 7) finds the consensus flow v * i which minimizes the overall distances between v * i and the flows within Ω(N ). The problem of Eq. ( 7) can be solved efficiently as an eigen-decomposition problem. Its solution can be obtained by computing the eigenvectors of the covariance matrix W T W, where the rows of W are w for all w ∈ Ω(N ). The desired smoothed flow vector corresponds to the eigenvector of the largest eigenvalue. Note that, all the w ∈ Ω(N ) are normalized to unit vectors to obtain the optimal solution. Recall the second assumption that the structure of the local point sets (Θ t = Θ(x, X t , w, r), t = 1, . . . , n) is preserved within a short time period t. Thus, the measurements of a local point set Θ t moving along w from Eq. 7 are homomorphic. Therefore, the shape of distribution of projections P t = P(Θ t , w) remain unchanged over time interval [1, t]. Let H t be a k-bin 1D histogram of projections P t at time t. The motion state of the point sets can be described by the following equation: where b is one bin of the histogram, and α(t) = βt in which β corresponds to the displacement of the histogram (or projections) from t to t + 1. Eq. ( 8) implies that the histogram is replicated from t = 1, . . . , n thanks to the temporal local structure and velocity consistency. Given a sequence of histograms H t (b), t = 1 . . . , n, our task is to estimate β and b such that Eq. ( 8) is satisfied for all t. Mathematically, it can be modelled as the followed minimization problem: To efficiently solve problem (9), the n-frame 1D histograms  L(t) = βt + b, with slope β and offset b. Note that the sought line L goes through the centres of the n-frame 1D histograms. In this regards, the optimal parameters β * and b * are obtained by Thanks to the Radon transform Deans [2007], which computes the volumetric integration in different angles at different positions in a continuous manner, problem (10) can be solved efficiently and globally by applying the Radon transform on M, as illustrated in the last column of Fig. 5. Three measurements are made along the line L * to categorize the point sets as static or dynamic. Firstly, the slope β * represents the magnitude of the motion speed, β * of a static point set is very small accordingly. Further, let s t = H t (L * ), t = 1, . . . , n be the values H t (b) on the line L * , two measurements are defined: Where S and E measure the strength and distribution homogeneity, respectively. A point set is considered to be static, if β * , S and E values are below their respective thresholds. Otherwise, the point set is assumed to be dynamic. Practical scenarios, in which the sizes and the speeds of objects may significantly vary (i.e. from pedestrians to trucks), impose to the scene analysis in a dynamic manner. A default size of the local bounding box may cover only a small (or respectively too large) part of the object. This problem can be effectively addressed by taking a relatively large bounding box with a radius-variance enclosing cylinder, where the cylinder radius is inversely proportional to the number of points within the enclosing cylinder. Our analysis algorithm is mostly driven by three parameters, namely the size of bounding box, its location, and the radius of the enclosing cylinder. To point out, we apply the bounding box (rather than an ellipsoid) for fast neighbourhood searching of Eq. ( 7) on the local flow field estimation. These three parameters can be reduced to two by taking a fixsized bounding box with the radius as a ratio of its size. A motion is considered as \"slow\" when the sequential point sets S are totally bounded by the pre-defined bounding box. Consequently, the slow motions are not problematic because the corresponding point sets remain in the same bounding box. Otherwise, the bounding box is translated along the motion flow direction to obtain a larger coverage. As soon as the consecutive frames have led to a coherent motion, the local neighbourhood is updated, as illustrated in Fig. 6. In this figure, the bounding box is supposed to cover 9 consecutive frames for the object's motion analysis, however, only 5 consecutive frames are covered within the given sized bounding box due to the large displacement of the moving object. In order to achieve a larger coverage, the bounding box is translated along the motion flow direction. Followed by, the enclosing cylinder is applied for the object's motion behaviour study. Regarding to the parameter reduction, it is sufficient to choose a radius that is 20% smaller than the size of the bounding box according to our experiments. Moreover, this radius is proportionally adapted to the distance between the object and the camera. Formally, we use a dynamic searching strategy along the flow direction. Let B = {B t , t = 1, . . . , f } be the assembly of f frames of point sets within a local bounding box. When a fast motion occurs, the bounding box (centred at x c ) covers f frames with f < n, where n is the objective frame length for motion analysis. Let P t (B t , w), t = 1, . . . , f be the projections of B along the motion direction w, and δ t = median(P t ), t = 1, . . . , f be the median values of projections of P t . The bounding box is translated to x t = x c + δ t w, until all n frames are covered. Starting with the camera ego-motion compensation, the ICPbased point cloud registration algorithms are applied to register the given n consecutive frames of point sets. Notably, robust ICP algorithms Fitzgibbon [2003]; Zhang and Singh [2016] are preferred to obtain precise camera motion estimation. According to our expertise, ICP registration on edge and plane feature points generally yields satisfactory results, similarly to Zhang and Singh [2016]. Taking the registered point sets as input, Algo. 1 is applied to discriminate the static and the dynamic points, and to estimate the motion flows of the dynamic points. For the sake of computational efficiency, the points from ground plane are detected and removed beforehand. Note that the detection of ground plane for the data acquired by a ground-vehicle is an almost solved problem [Douillard et al., 2011;Zermas et al., 2017]. In step 4, the enclosing cylinder radius is defined as r = 0.4(1 + d/D), where d is the object to camera distance and D is the camera's maximum data acquisition distance (e.g. D = 100 meters for Velodyne HDL-64 [Lidar, 2016] 3D laser scanner). In step 7, τ S is defined as 40% of the total number of neighbours within the enclosing cylinder (also known as the sum value of the 2D histogram M). τ β = 0.175 denotes that the slope of L * is 10 degree. τ E = 1.8 is empirically studied and used for all our experiments. We recall that the Radon transform calculates the volumetric integration in both angular and positional domains. Thus, its maximum response complies to the sought optimal solution of problem (10). In Fig. 5 Col. 2, the 1D histograms from dynamic scene part have shifting effects along the flow direction, as expected. Differently, these histograms tend to overlap with each other for the static scene parts. These phenomena lead to the different properties (refer to the above discussions in Section III-B) of the motion line L * of static and dynamic points. 4 Fit an enclosing cylinder Θ(x i , X, v, r). Project cylinder points to axis v using Eq. ( 4), and compute histograms Ht, t = 1, . . . , n to construct M. Compute the slope β * of L * using Radon transform on M, motion strength S and stability E using Eq. ( 11). In this section, we introduce our 3D SFC algorithm to further analyse the object's motion behaviour. By obtaining a set of dynamic points and their corresponding motion flows, as discussed in the above Section III, the 3D SFC intends to cluster them into multiple subsets w.r.t. their motion properties, i.e. similar motion speed, alike motion direction, and small spatial distance. Our clustering process uses the information from space subset S as well as their corresponding assignment vectors Z. On the one hand, we rely on the assumption that the vectors from one cluster are self-expressive. In other words, a flow vector can be closely approximated by the linear combination of the other flow vectors from the same cluster. On the other hand, we ensure that the clustered vector fields have bounded space subset within the predefined radius. Let X = [x 1 , . . . x j , . . . x n ] and W = [w 1 , . . . w j , . . . w n ] are 3 × n matrices of the point set and the corresponding flow vectors, the self-expressive sparse representation (similar to Elhamifar and Vidal [2013]) can be written as where the sparse n × n matrix C = [c 1 , . . . c j , . . . c n ] with c jj = 0 to avoid trivial solutions, for all j = 1, . . . n. Similarly, for a predefined squared radius bound r (where the sparsity comes from), the bounded space subset is ensured by enforcing the constraint Therefore, the sparsity-constraint relaxed optimization problem for flow clustering can be written as minimize This is a convex problem, whose optimal solution can be found by using the second order cone programming Boyd and Vandenberghe [2004]. In fact, its equivalent problem as the semi-definite programming is given by minimize ) where s ij are the elements of S. In practical scenarios, the flow data might be contaminated by noise or outliers. Let where e j ∈ IR 3 is the noise or outlier entry of noise free data w 0 j . Replacing Eq. ( 12) with Eq. ( 16), we have Due to the local structure persistence and temporal flow speed consistency assumptions, the sought sparse representation from the current frame is valid for the neighbor frames. Therefore, the sparse subspace clustering problem of Eq. ( 15) can be reformulated as: ) where X t and W t are the 3D points and their flow vectors at frame t, respectively. In Eq. ( 18), E w = λ 1 n j=1 e w and E x = λ 2 n j=1 e x are energy terms with weight parameters λ 1 and λ 2 to control the influence of spatial (i.e. the 3D point coordinate) and temporal (i.e. the motion flow) factors, and we simply set λ 1 = 1 and λ 2 = 1. Note that the squared radius bound w and x are constrained to be non-negative, but not predefined. Similarly, Eq. ( 18) can be solved as a semi-definite programming problem. 3 K-mean spectral clustering on G. Getting the sparse subspace representation matrix C, a sparse symmetric similarity graph, which stands for the connectivity among the flows, can be constructed as G = |C| + |C| T . To group the flows into their corresponding motions, a spectral clustering approach [Atev et al., 2010;Ng et al., 2002] can be applied on G to segment the motion flows into their individual groups, see Fig. 7. In this figure, the top-left image shows a sequence of registered 3D point clouds where several moving objects exist, specifically two moving cars, three walking pedestrians, and a cyclist. The bottom-left image is the zoom-in view of the detected motion flows on a moving car. The middle image demonstrates a clustered connectivity graph G which reveals the relationship between each 3d flow. Note that G is derived from the sparse representation matrix C, it is expected that each independent motion forms one diagonal block with sparse non-zero entries. The size of the diagonal block is determined by the cluster's element number relating to the object's size and the density of point set. To this end, the right image shows the color-coded motion cluster in a more illustrative manner. The SFC algorithm consists of three major steps (see Algo. 2) which are implemented based on the CVX Grant and Boyd [2008] optimization toolbox. In the sparse optimization step, a point to point distance graph is applied to enforce the spatial closeness of the selected sparse representation elements, such that Eq. ( 18) becomes Where operator (•) stands for the dot product, and τ d is the point-to-point spatial distance threshold. Two major remarks on spatial distance constraint can be made: a) It is more meaningful to use sparse representation only on the local neighbourhood. b) Exploiting the sparsity of C improves the algorithm's robustness and computational efficiency. In step 2 of Algo. 2, a sparse symmetric similarity graph T is constructed. Since G encodes the connectivity information among the flows, a K-mean spectral clustering is employed to group the flow clusters. In fact, K can be determined by finding the number of graph components via the analysis of the eigenspectrum of the Laplacian matrix of G [Von Luxburg, 2007]. However, other model selection techniques [Brox and Malik, 2010] should be employed when there are connections between points in different subspaces. In the following experiments, we provide the number of motions as an input to all the algorithms for fair comparison. To emphasize, the proposed SFC does not rely on feature tracking and feature trajectory construction (unlike [Elhamifar and Vidal, 2013;Hu et al., 2014;Jiang et al., 2016]), making it more appropriate for highly dynamic environment motion analysis. Moreover, the SFC algorithm, which is proposed under the robust sparse subspace representation framework, offers new research perspectives for vector field analysis. We evaluate the proposed algorithms by conducting extensive experiments on the challenging real-world KITTI benchmark Geiger et al. [2013] with rapidly changing environments. The seven representative datasets (namely Campus, Cola Truck, Junction, Market, Pedestrian, Red Light, and Station) have been carefully selected to cover a wide range of moving  objects in terms of quantity, size, speed, shape, occlusion, etc. To test the flexibility of the algorithms in terms of camera motion, the Campus, Pedestrian and Station sequences are acquired from a static camera set-up, while the other sequences are acquired when the camera is moving. The detailed results are synthesized in Table I Col. 2-3 and Table III Col. 2-5. The performances with the state-of-the-art methods are assessed by using the Sensitivity and Specificity metrics [Fawcett, 2006], defined as follows:  For comparison with MS-based methods, the misclassification rate metric suggested by Elhamifar and Vidal [2013]; Hu et al. [2014] is adopted. All the experiments have been conducted on a machine with Intel Quad Core i7-2.7GHz, 32GB Memory using MATLAB. We compare the proposed 3D-based Moving Object Detection algorithm (3D-MOD) against the four representative algorithms available in the literature. Remind that the 2D-SMR, 2D-SSC and 3D-SSC are feature-based motion segmentation algorithms cluster the feature trajectories into their corresponding motions. We define: True Positive -if only a motion trajectory is NOT classified as background motion, and True Negative -when a background trajectory is classified as background motion. Here, we consider the feature trajectories belong to the static scene parts as background motion. When several motions are involved, although a feature trajectory might not be correctly classified into its corresponding motion, it is yet considered as a true positive. Table I summarizes the quantitative evaluation of 2D-SMR-J1 Hu et al. [2014], 2D-SMR-J2 Hu et al. [2014], 2D-SSC Elhamifar and Vidal [2013], 3D-SSC Jiang et al. [2016] and 3D-MOD on the seven representative datasets by using the Sensitivity and Specificity metrics. There are several remarks listed as follows: a. Both the 2D-SSC and the 3D-SSC algorithms achieve quite good performances in terms of sensitivity. Meanwhile, the 3D-SSC has much better specificity but rather low computational efficiency. b. Although the 2D-SMR based approaches have the worst performance in both sensitivity and specificity, such approaches have the best time performance which offers great potential in real-time applications. c. Overall, the 3D-MOD accomplishes quite decent performances in both sensitivity and specificity. Precisely, the 3D-MOD has a more superior averaged sensitivity, as well as a remarkably higher averaged specificity. d. The 3D-based methods (i.e. 3D-SSC and 3D-MOD) exhibit very stable performances, especially much higher specificity, thanks to their insensitivity to perspective projection effects. e. Regarding the computational efficiency, our 3D-MOD approach provides a compromised solution. In addition, it can be easily parallelized and boosted if online MOD is required. Further, we adopt the mean and median Misdetection Error metrics defined by as in Elhamifar and Vidal [2013]; Hu et al. [2014] for MOD performance evaluation, refer to Table II. Illustratively, the corresponding box-plot statistical comparisons are provided as in Fig. 8. Similarly, the 3D-SSC and 3D-MOD have noteworthy better performances than other methods due to their persistent high specificity. To point out, the 3D-MOD outperforms the other methods with clearly lower median misdetection rate as well as much higher robustness, as shown in Fig. 8. Moreover, the 3D-MOD is compared against the Object Scene Flow (OSF) Menze et al. [2018] algorithm, as concluded in Table III. Since the OSF method produces pixel-level dense moving object detection and segmentation, it is more appropriate to compare their performances in a dense manner. In this regards, the 3D Region Growing Mühlenbruch et al. [2006]    3D-MOD, is applied to densely segment the moving objects. Thus, both the Sensitivity and Specificity are computed by using dense segmentation of 3D point clouds. From this table, we observe that the 3D-MOD is not only faster, but also consistently exhibits a much higher sensitivity with just a slightly lower specificity. To conclude, the main reasons that the 3D-MOD surpasses the state-of-the-art methods are: a) The 3D-MOD relies on a pre-registration of point clouds, while the motion segmentation-based methods utilize the raw feature trajectories without ego-motion compensation. b) The 3D-MOD interprets the motions by using high quality 3D data, while the OSF estimates a low-precision 3D scene structure by using stereo vision techniques. c) The 3D-MOD analyses the 3D motion behaviours under local flows consistency assumption, which addresses the problem in essence. Quantitatively, we utilize the Misclassification Rate (same as η) to compare the performances of the different algorithms, as shown in Fig. 9. In most cases, the 2D-based approaches  , respectively. The right images are outcomes of 3Dbased approaches, namely the 3D-SSC [Jiang et al., 2016], the 3D-SMR [Jiang et al., 2017b] and the proposed 3D-SFC. Red boxes highlight the undetected or incorrectly segmented motions. achieve much higher misclassification rate, while the 3Dbased algorithms obtain relatively lower misclassification rate. Furthermore, our 3D-SFC exceptionally outperforms the compared algorithms on the evaluated datasets. Qualitatively, Fig. 10 illustrates the motion segmentation results of the compared algorithms on the Market sequence. Left column images exemplify that the 2D-based motion segmentation results are quite unsatisfactory, while the right column images of the 3D-based approaches manifest much better outcomes. Particularly, in the bottom-right image, all the moving objects are detected correctly, including the black car in the middle of the scene and the walking pedestrian on the left side. The excellent performance of the proposed 3D-Fig. 11. Top image is the full scene 3D reconstruction by using Zhang and Singh [2016] of Market sequence where numerous moving objects occur. The zoom-in regions show the immense artifacts from the walking pedestrians. Bottom image is our static-map which has distinctively higher quality. a) The 3D-SFC classifies the detected motion flows from 3D-MOD, which contributes to the discard of most background features. b)The 3D-SFC is proposed under the sparse representation framework with extra spatial closeness constraint, which produces a very reliable similarity graph for spectral clustering. We conducted multiple experiments on the KITTI dataset and obtained quite better static-maps than the other approaches, as proved with the detailed tests and measures presented below. Fig. 11 shows the challenging Market sequence which contains a large amount of moving objects. The staticmap produced by our framework is of significantly better quality because our framework is not sensitive to light changes, occlusions, slow or very fast motions, etc. Remarkably, top images in Fig. 11 contains serious \"ghost\" artefacts caused by the trajectories of moving objects, which not only degrades the visual quality but also defects the functionality of the reconstructed 3D map. For instance, these \"ghost\" artefacts occlude the ground areas and the lane markings in road surface, making the automatic or manual labelling in High Definition Map (HD-Map) production more difficult. Moreover, performances of map-based localization algorithms [Levinson et al., 2007;Magnusson et al., 2007;Wan et al., 2018] are expected to deteriorate due to the heavily-noise corrupted point cloud map. By applying the proposed framework, the bottom images in Fig. 11 demonstrate that the 3D point cloud map contains only the stable objects, so-called static map. Such static map offers great potentials in applications such as city scene modelling [Babahajiani et al., 2017;Fan et al., 2009], automatic lane marking extraction in HD-Map production [Guan et al., 2015;Prochazka et al., 2019], landmark-based localization in autonomous driving [Lu et al., 2019], etc. Moreover, the top-row images of Fig. 12 and 13 illustrate the superior quality of the synthetic images rendered by projecting the textured 3D point cloud of static maps onto a virtual camera coordinate. As can be seen from the bottomrow images of Fig. 12 and 13, many walking pedestrians are captured by the vehicle's camera. However, for some specific applications such as the Google Street View [Anguelov et al.,Fig. 12. Synthetic image generation of market sequence by using the reconstructed static map. Top images illustrate the static scene imaginary of the market area, while the bottom images are \"contaminated\" by the moving car and the numerous walking pedestrians. Fig. 13. Synthetic image generation of station sequence: top image is the rendered static scene imaginary by using the reconstructed static map. By comparing to the bottom real camera captured image, we notice that large moving objects (such as the moving car and train) are correctly detected and removed. 2010; Mao et al., 2011], it is preferable to have a clean view of the city scene. Noteworthy, the synthetic image generation is, in essence, a key component of video inpainting technique [Newson et al., 2014;Zhang et al., 2019]. Moreover, these synthetic high quality images of static scenes can be used as reference images to label the moving objects, which makes the moving object annotation far more efficient and intelligent. Apart from the static map reconstruction, getting the clustered motion trajectories from our framework, the 3D reconstruction of moving objects can be obtained by registering the observed sparse point clouds during their motions. Fig. 14 shows two reconstructed rigidly moving objects. Thanks to the proposed 3D-SFC, the detected moving objects can be separated according to their specific motion subspace. The detected moving objects are then individually registered with texture mapping to produce photo-realistic 3D modelling [Jiang et al., 2017a]. For more experimental results, readers are recommended to view this video (https://youtu.be/LewA8Lhn5Xo). We have proposed an original 3D Flow Field Analysis (3D-FFA) algorithm for 3D Moving Object Detection (3D-MOD) under the motion consistency assumption of a local neighbourhood. We further present a novel 3D Sparse Flow Clustering (3D-SFC) approach based on the self-expressiveness property of motion flow subspace as well as the spatial closeness constraint. By integrating the proposed 3D-MOD and 3D-SFC algorithms, we show that our framework is not merely robust, efficient and accurate, but also allows photo-realistic static-map and dynamic object reconstructions by using a 2D-3D moving camera system. In many aspects, both the 3D-MOD and 3D-SFC algorithms outperform the state-of-theart methods since we have compared all these techniques on comprehensive highly dynamic real-world KITTI datasets, for which they consistently exhibit better accuracy, lower misclassication and misdetection rates, and consequently yield very high quality 3D reconstructions of static-maps as well as moving objects. In addition, the proposed framework offers great potentials in 3D city scene modelling, robot navigation and many other autonomous driving applications. As for future perspectives, since the proposed 3D-FFA algorithm makes the assumption of linear motion, it may failed to detect pure rotation motions. Thus, a complementary algorithm in dealing with pure rotation motions is preferred. Moreover, it is interesting to produce higher resolution synthetic image sequence by incorporating the image inpainting techniques. Furthermore, recent advances, such as Point FlowNet3D [Behl et al., 2019;Liu et al., 2019a], achieve very interesting results in estimating 3D scene flows, which should benefit to a better performance in motion flow field analysis as long as higher computational cost is inessential." ;
        qont:metadata     [ dct:language  "en" ] ;
        scilake:has_part  <http://scilake-project.eu/res/7d2c44be#offset_29751_29858> , <http://scilake-project.eu/res/7d2c44be#offset_10530_14323> , <http://scilake-project.eu/res/7d2c44be#offset_31605_32630> , <http://scilake-project.eu/res/7d2c44be#offset_45897_47704> , <http://scilake-project.eu/res/7d2c44be#offset_35544_36669> , <http://scilake-project.eu/res/7d2c44be#offset_27631_29640> , <http://scilake-project.eu/res/7d2c44be#offset_22842_24944> , <http://scilake-project.eu/res/7d2c44be#offset_18074_20730> , <http://scilake-project.eu/res/7d2c44be#offset_41972_42278> , <http://scilake-project.eu/res/7d2c44be#offset_42279_45896> , <http://scilake-project.eu/res/7d2c44be#offset_0_49> , <http://scilake-project.eu/res/7d2c44be#offset_29641_29750> , <http://scilake-project.eu/res/7d2c44be#offset_32631_33812> , <http://scilake-project.eu/res/7d2c44be#offset_24945_27630> , <http://scilake-project.eu/res/7d2c44be#offset_29859_31604> , <http://scilake-project.eu/res/7d2c44be#offset_14324_18073> , <http://scilake-project.eu/res/7d2c44be#offset_1134_10529> , <http://scilake-project.eu/res/7d2c44be#offset_592_1133> , <http://scilake-project.eu/res/7d2c44be#offset_33813_35543> , <http://scilake-project.eu/res/7d2c44be#offset_21414_22841> , <http://scilake-project.eu/res/7d2c44be#offset_50_591> , <http://scilake-project.eu/res/7d2c44be#offset_36670_40503> , <http://scilake-project.eu/res/7d2c44be#offset_40504_41971> , <http://scilake-project.eu/res/7d2c44be#offset_20731_21413> .

<http://scilake-project.eu/res/7d2c44be#offset_41972_42278>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "a) The 3D-SFC classifies the detected motion flows from 3D-MOD, which contributes to the discard of most background features. b)The 3D-SFC is proposed under the sparse representation framework with extra spatial closeness constraint, which produces a very reliable similarity graph for spectral clustering." ;
        nif:beginIndex            "41972"^^xsd:nonNegativeInteger ;
        nif:endIndex              "42278"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "SFC mainly comes from:" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_14324_18073>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "2D Lidar-based Approaches: 2D lidar sensors (or single layer laser scanner) are widely used in industrial robots [Wang et al., 2015] or ADAS applications [Ziebinski et al., 2017] for object detection and tracking. Traditional approaches [Mertz et al., 2013;Wang et al., 2007Wang et al., , 2015] ] proposed to detect and track the moving objects along side with the simultaneous localization and mapping (SLAM) framework. The moving objects are detected by discriminating their temporal and spatial displacement. However, such method is limited to 2D laser scanner with the assumption of flat ground plane and a specific height range of moving object. 3D Lidar-based Approaches: 3D lidar (or multi-layered laser scanner) are very popular in autonomous driving applications [Levinson et al., 2011] nowadays. Taking the advantage of precise 3D point clouds, [Steinhauser et al., 2008;Sualeh and Kim, 2019;Wang et al., 2012;Ye et al., 2016] proposed to detect the possible moving objects (e.g. cars or pedestrians) and track them as motion candidates. Such approaches are trivial but require precise classifiers or feature descriptors ( [Dewan et al., 2016]) for object recognition, which is usually impractical due to the sparse point cloud and object occlusions. Apart from the above geometrical analysis-based approaches, [Börcs et al., 2017;Engelcke et al., 2017] applied the deep learning techniques resulting in more precise object recognition capability. Without relying on the object knowledge, [Asvadi et al., 2015;Azim and Aycard, 2012] utilized the occupancy grid map to statically predict and track the moving objects. Nonetheless, designing the occupancy grid size and the selection of statistical model are empirically difficult. Recently, Deep Learning -based approaches [Behl et al., 2019;Fan and Yang, 2019;Liu et al., 2019a,b] presented interesting results on 3D scene flow estimation thanks to the recent advances in computational resources and largescale training dataset. In particular, [Fan and Yang, 2019] proposed a series of point-based recurrent neural networks, i.e. the PointRNN, the PointGRU, and the PointLSTM, for dynamic point cloud forecasting via flow predicting. Such approaches adopt the spatio-temporally-local correlation to aggregate the point features and their states according to the point coordinates. [Liu et al., 2019a] focus on 3D action recognition, dynamic point cloud segmentation, and scene flow estimation with multiple frames. The proposed FlowNet-based methods apply an end-to-end model for both point feature association and flow estimation. Other approaches [Cho et al., 2014;Takabe et al., 2016] intended to fuse the image and lidar observations for MOD using photometric and depth consistencies. [Rashed et al., 2019] made use of both the camera and lidar for robust MOD in low-light autonomous driving environments. Although the deep learning-based approaches show promising results, it is difficult to collect massive training data and to have heavy computational resources during the training process. To summarize, unlike the above discussed methods, the proposed algorithms neither rely on feature tracking across the frame sequence contributing to their robustness to occlusions, nor require exhaustive machine learning training process making them being handy and easy to implement. Our MOD algorithm directly detects and segments the motion flows using raw 3D point cloud sequence without texture information. Although a rough 3D point cloud registration step is required for ego-motion compensation, the traditional ICPbased registration techniques [Fitzgibbon, 2003] are sufficient. Moreover, our method is very generic in detecting moving objects in terms of size, speed and direction." ;
        nif:beginIndex            "14324"^^xsd:nonNegativeInteger ;
        nif:endIndex              "18073"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "B. Lidar-based MOD" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_0_49>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "Moving Object Detection by 3D Flow Field Analysis" ;
        nif:beginIndex            "0"^^xsd:nonNegativeInteger ;
        nif:endIndex              "49"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "Moving Object Detection by 3D Flow Field Analysis" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "title" .

<http://scilake-project.eu/res/7d2c44be#offset_29641_29750>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "Project cylinder points to axis v using Eq. ( 4), and compute histograms Ht, t = 1, . . . , n to construct M." ;
        nif:beginIndex            "29641"^^xsd:nonNegativeInteger ;
        nif:endIndex              "29750"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "5" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_592_1133>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés." ;
        nif:beginIndex            "592"^^xsd:nonNegativeInteger ;
        nif:endIndex              "1133"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés." ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "abstract" .

<http://scilake-project.eu/res/7d2c44be#offset_32631_33812>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "Getting the sparse subspace representation matrix C, a sparse symmetric similarity graph, which stands for the connectivity among the flows, can be constructed as G = |C| + |C| T . To group the flows into their corresponding motions, a spectral clustering approach [Atev et al., 2010;Ng et al., 2002] can be applied on G to segment the motion flows into their individual groups, see Fig. 7. In this figure, the top-left image shows a sequence of registered 3D point clouds where several moving objects exist, specifically two moving cars, three walking pedestrians, and a cyclist. The bottom-left image is the zoom-in view of the detected motion flows on a moving car. The middle image demonstrates a clustered connectivity graph G which reveals the relationship between each 3d flow. Note that G is derived from the sparse representation matrix C, it is expected that each independent motion forms one diagonal block with sparse non-zero entries. The size of the diagonal block is determined by the cluster's element number relating to the object's size and the density of point set. To this end, the right image shows the color-coded motion cluster in a more illustrative manner." ;
        nif:beginIndex            "32631"^^xsd:nonNegativeInteger ;
        nif:endIndex              "33812"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "B. Spectral Clustering" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_36670_40503>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "We compare the proposed 3D-based Moving Object Detection algorithm (3D-MOD) against the four representative algorithms available in the literature. Remind that the 2D-SMR, 2D-SSC and 3D-SSC are feature-based motion segmentation algorithms cluster the feature trajectories into their corresponding motions. We define: True Positive -if only a motion trajectory is NOT classified as background motion, and True Negative -when a background trajectory is classified as background motion. Here, we consider the feature trajectories belong to the static scene parts as background motion. When several motions are involved, although a feature trajectory might not be correctly classified into its corresponding motion, it is yet considered as a true positive. Table I summarizes the quantitative evaluation of 2D-SMR-J1 Hu et al. [2014], 2D-SMR-J2 Hu et al. [2014], 2D-SSC Elhamifar and Vidal [2013], 3D-SSC Jiang et al. [2016] and 3D-MOD on the seven representative datasets by using the Sensitivity and Specificity metrics. There are several remarks listed as follows: a. Both the 2D-SSC and the 3D-SSC algorithms achieve quite good performances in terms of sensitivity. Meanwhile, the 3D-SSC has much better specificity but rather low computational efficiency. b. Although the 2D-SMR based approaches have the worst performance in both sensitivity and specificity, such approaches have the best time performance which offers great potential in real-time applications. c. Overall, the 3D-MOD accomplishes quite decent performances in both sensitivity and specificity. Precisely, the 3D-MOD has a more superior averaged sensitivity, as well as a remarkably higher averaged specificity. d. The 3D-based methods (i.e. 3D-SSC and 3D-MOD) exhibit very stable performances, especially much higher specificity, thanks to their insensitivity to perspective projection effects. e. Regarding the computational efficiency, our 3D-MOD approach provides a compromised solution. In addition, it can be easily parallelized and boosted if online MOD is required. Further, we adopt the mean and median Misdetection Error metrics defined by as in Elhamifar and Vidal [2013]; Hu et al. [2014] for MOD performance evaluation, refer to Table II. Illustratively, the corresponding box-plot statistical comparisons are provided as in Fig. 8. Similarly, the 3D-SSC and 3D-MOD have noteworthy better performances than other methods due to their persistent high specificity. To point out, the 3D-MOD outperforms the other methods with clearly lower median misdetection rate as well as much higher robustness, as shown in Fig. 8. Moreover, the 3D-MOD is compared against the Object Scene Flow (OSF) Menze et al. [2018] algorithm, as concluded in Table III. Since the OSF method produces pixel-level dense moving object detection and segmentation, it is more appropriate to compare their performances in a dense manner. In this regards, the 3D Region Growing Mühlenbruch et al. [2006]    3D-MOD, is applied to densely segment the moving objects. Thus, both the Sensitivity and Specificity are computed by using dense segmentation of 3D point clouds. From this table, we observe that the 3D-MOD is not only faster, but also consistently exhibits a much higher sensitivity with just a slightly lower specificity. To conclude, the main reasons that the 3D-MOD surpasses the state-of-the-art methods are: a) The 3D-MOD relies on a pre-registration of point clouds, while the motion segmentation-based methods utilize the raw feature trajectories without ego-motion compensation. b) The 3D-MOD interprets the motions by using high quality 3D data, while the OSF estimates a low-precision 3D scene structure by using stereo vision techniques. c) The 3D-MOD analyses the 3D motion behaviours under local flows consistency assumption, which addresses the problem in essence." ;
        nif:beginIndex            "36670"^^xsd:nonNegativeInteger ;
        nif:endIndex              "40503"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "A. Motion Detection Evaluation" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_42279_45896>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "We conducted multiple experiments on the KITTI dataset and obtained quite better static-maps than the other approaches, as proved with the detailed tests and measures presented below. Fig. 11 shows the challenging Market sequence which contains a large amount of moving objects. The staticmap produced by our framework is of significantly better quality because our framework is not sensitive to light changes, occlusions, slow or very fast motions, etc. Remarkably, top images in Fig. 11 contains serious \"ghost\" artefacts caused by the trajectories of moving objects, which not only degrades the visual quality but also defects the functionality of the reconstructed 3D map. For instance, these \"ghost\" artefacts occlude the ground areas and the lane markings in road surface, making the automatic or manual labelling in High Definition Map (HD-Map) production more difficult. Moreover, performances of map-based localization algorithms [Levinson et al., 2007;Magnusson et al., 2007;Wan et al., 2018] are expected to deteriorate due to the heavily-noise corrupted point cloud map. By applying the proposed framework, the bottom images in Fig. 11 demonstrate that the 3D point cloud map contains only the stable objects, so-called static map. Such static map offers great potentials in applications such as city scene modelling [Babahajiani et al., 2017;Fan et al., 2009], automatic lane marking extraction in HD-Map production [Guan et al., 2015;Prochazka et al., 2019], landmark-based localization in autonomous driving [Lu et al., 2019], etc. Moreover, the top-row images of Fig. 12 and 13 illustrate the superior quality of the synthetic images rendered by projecting the textured 3D point cloud of static maps onto a virtual camera coordinate. As can be seen from the bottomrow images of Fig. 12 and 13, many walking pedestrians are captured by the vehicle's camera. However, for some specific applications such as the Google Street View [Anguelov et al.,Fig. 12. Synthetic image generation of market sequence by using the reconstructed static map. Top images illustrate the static scene imaginary of the market area, while the bottom images are \"contaminated\" by the moving car and the numerous walking pedestrians. Fig. 13. Synthetic image generation of station sequence: top image is the rendered static scene imaginary by using the reconstructed static map. By comparing to the bottom real camera captured image, we notice that large moving objects (such as the moving car and train) are correctly detected and removed. 2010; Mao et al., 2011], it is preferable to have a clean view of the city scene. Noteworthy, the synthetic image generation is, in essence, a key component of video inpainting technique [Newson et al., 2014;Zhang et al., 2019]. Moreover, these synthetic high quality images of static scenes can be used as reference images to label the moving objects, which makes the moving object annotation far more efficient and intelligent. Apart from the static map reconstruction, getting the clustered motion trajectories from our framework, the 3D reconstruction of moving objects can be obtained by registering the observed sparse point clouds during their motions. Fig. 14 shows two reconstructed rigidly moving objects. Thanks to the proposed 3D-SFC, the detected moving objects can be separated according to their specific motion subspace. The detected moving objects are then individually registered with texture mapping to produce photo-realistic 3D modelling [Jiang et al., 2017a]. For more experimental results, readers are recommended to view this video (https://youtu.be/LewA8Lhn5Xo)." ;
        nif:beginIndex            "42279"^^xsd:nonNegativeInteger ;
        nif:endIndex              "45896"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "C. Static-map and Rigid Object Reconstruction" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_10530_14323>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "Feature Trajectory Analysis-based Approaches: feature trajectories are the one of the most important clues of object motions. In this context, Motion Segmentation (MS) techniques, such as the Generalized Principal Component Analysis (GPCA) [Vidal et al., 2005], RANSAC-based MS [Yan and Pollefeys, 2007], and Agglomerative Subspace Clustering [Rao et al., 2010], are proposed to group the feature trajectories according to the objects' motion subspaces. The GPCA is the representative approach that offers an algebro-geometric solution to the MS problem without the knowledge of subspace number and dimension, by representing the subspaces with a set of homogeneous polynomials. As claimed by the authors, the GPCA also provides a robust initialization to iterative techniques such as K-subspaces or Expectation Maximization algorithms. However, the determination of number of clusters and their dimensions only works for noise free data in practice. Differently, [Elhamifar and Vidal, 2013] proposed the groundbreaking Sparse Subspace Clustering (2D-SSC) algorithm relying on the self-representation property of the affine motion subspace. The 2D-SSC assumes that one feature trajectory can be represented by other feature trajectories from the same motion subspace. By incorporating the sparsity constraint on the relaxed 1 optimization, the 2D-SSC offers a robust solution to MS with outliers and achieves significantly better performances. However, the computational complexity of the 2D-SSC is proportional to the cubic of the problem size, and is therefore expensive for large scale data. As inspired, [Hu et al., 2014] proposed a SMooth Representation (2D-SMR) clustering model which outperforms the existing methods in literature by enforcing the grouping effects of the motion subspaces from image feature trajectories. To overcome the perspective projection problem of the image feature trajectories, [Jiang et al., 2016] proposed a 3D databased Sparse Subspace Clustering (3D-SSC) algorithm which achieves comparative performances against its 2D counterparts without affine motion constraint. This algorithm relies on the consistency of the tracked trajectories and is therefore sensitive to lost tracking situations and partial occlusions. To improve its robustness, [Jiang et al., 2017b] proposed a 3D-SMR algorithm which jointly benefits from the 2D-SMR in scalable feature size and tracking correspondence. In a more sophisticated manner, [Keuper et al., 2018] incorporate the low-level feature trajectory and high-level object recognition cues to achieve better performance. Inherently, feature trajectory construction is sensitive to image noise and environment change, making such approaches limited to slow camera motion and temporally consistent lighting conditions. Motion Flow Analysis-based Approaches: the motion flow, which encodes the motion magnitude and direction of the image pixel, is widely used in moving object discovery by analyzing the flow field discontinuities, such as [Huang et al., 2018;Mémin and Pérez, 2002;Yokoyama and Poggio, 2005]. The piecewise-smooth flow field are segmented by using Hierarchical [Mémin and Pérez, 2002], Level-set [Mitiche and Sekkati, 2006], or Graph-cut [Wedel et al., 2009] segmentation algorithms. More recently, [Ma et al., 2019;Menze et al., 2018] intended to detect and analyze the rigidly moving objects as Object Scene Flow (OSF) using stereo vision setup. Inspired by OSF [Menze and Geiger, 2015], Kochanov et al. [Kochanov et al., 2016] proposed to detect and segment the moving objects by propagating the OSF output to construct the static-map. The OSF-based approaches usually achieve more precise results, however, they are sensitive to the environment changes and require precise object motion model estimations." ;
        nif:beginIndex            "10530"^^xsd:nonNegativeInteger ;
        nif:endIndex              "14323"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "A. Image-based MOD" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_29751_29858>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "Compute the slope β * of L * using Radon transform on M, motion strength S and stability E using Eq. ( 11)." ;
        nif:beginIndex            "29751"^^xsd:nonNegativeInteger ;
        nif:endIndex              "29858"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "6" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_33813_35543>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "The SFC algorithm consists of three major steps (see Algo. 2) which are implemented based on the CVX Grant and Boyd [2008] optimization toolbox. In the sparse optimization step, a point to point distance graph is applied to enforce the spatial closeness of the selected sparse representation elements, such that Eq. ( 18) becomes Where operator (•) stands for the dot product, and τ d is the point-to-point spatial distance threshold. Two major remarks on spatial distance constraint can be made: a) It is more meaningful to use sparse representation only on the local neighbourhood. b) Exploiting the sparsity of C improves the algorithm's robustness and computational efficiency. In step 2 of Algo. 2, a sparse symmetric similarity graph T is constructed. Since G encodes the connectivity information among the flows, a K-mean spectral clustering is employed to group the flow clusters. In fact, K can be determined by finding the number of graph components via the analysis of the eigenspectrum of the Laplacian matrix of G [Von Luxburg, 2007]. However, other model selection techniques [Brox and Malik, 2010] should be employed when there are connections between points in different subspaces. In the following experiments, we provide the number of motions as an input to all the algorithms for fair comparison. To emphasize, the proposed SFC does not rely on feature tracking and feature trajectory construction (unlike [Elhamifar and Vidal, 2013;Hu et al., 2014;Jiang et al., 2016]), making it more appropriate for highly dynamic environment motion analysis. Moreover, the SFC algorithm, which is proposed under the robust sparse subspace representation framework, offers new research perspectives for vector field analysis." ;
        nif:beginIndex            "33813"^^xsd:nonNegativeInteger ;
        nif:endIndex              "35543"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "C. Implementation Details and Discussions" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_21414_22841>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "Let a collection of n-consecutive point sets be S = {X t , t = 1, . . . , n}. For t = 1, . . . , n − 1, we compute the point-wise flow sets Z = {W t , t = 1, . . . , n − 1} which represent the motion of points over time t, as follow: Recall the definition of Eq. ( 1) and Eq. ( 2), the element-wise subtraction is performed to estimate the motion flows between frame t and frame t + 1. Due to the noisy observation of point set and the incorrect point-pair association, see Fig. 2 Block 2, the estimated motion flow using Eq. ( 6) can be incorrect. Therefore, by taking the locally homogeneous assumption of neighbouring flow vectors, we perform the smoothing of vector field by updating each w i ∈ W t as where v * i is the desired smooth flow vector to replace w i . Refer to Eq. ( 2), N = N (x i , X t , r) is the neighbourhood (within the radius r) that defines the local flow field Ω(N ). Actually, Eq. ( 7) finds the consensus flow v * i which minimizes the overall distances between v * i and the flows within Ω(N ). The problem of Eq. ( 7) can be solved efficiently as an eigen-decomposition problem. Its solution can be obtained by computing the eigenvectors of the covariance matrix W T W, where the rows of W are w for all w ∈ Ω(N ). The desired smoothed flow vector corresponds to the eigenvector of the largest eigenvalue. Note that, all the w ∈ Ω(N ) are normalized to unit vectors to obtain the optimal solution." ;
        nif:beginIndex            "21414"^^xsd:nonNegativeInteger ;
        nif:endIndex              "22841"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "A. Smooth Flow Vector Estimation" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_18074_20730>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "where w i ∈ R 3 , be the set of flow vectors associated to X. The 3D vector field Ω defined by X and W is notated as Ω : X → W. Given a sequence of point sets from a dynamic scene, we define S = {X t , t = 1, . . . , n} as the collection of multiple observed point sets that evolve over time t. Likewise, Z = {W t , t = 1, . . . , n − 1} is the collection of flow vectors associated to S. For two 3D point sets A and B, the vector field Ω : A → W can be obtained by the element-wise subtraction between the two point sets. We define the element-wise subtraction operation A B as where x i is an element of A, and The subtraction x i −y i defines the flow vector w i . The closest point function N (x, B) is defined as In a similar manner, the nearest neighbourhood set of points centred at x within a radius r is given by (3) We also define P(S, w), the projection of set S on the flow vector w (similarly, P(x, w) for point x), such that Projection Value We refer the illustrative examples of Eq. ( 4) to Fig. 3 and Fig. 4. In Fig. 3, a set of 3D points are projected onto the given 3D vector and the projection values are statistically represented as an n-bin one dimensional histogram. Similarly, in Fig. 4, two 3D points x 1 , x 2 are projected onto w c as two blue dots p 1 , p 2 . Note that the projection of a three dimensional point to the given 3D vector axis corresponds to its foot of perpendicular to the 3D vector, but we only take into account its scalar abscissa p on the axis. The origin of the axis is a specified 3D point, e.g. the mean values of the 3D point set. Since the mathematical representation of a 3D point is similar to a 3D vector, the projection of one 3D vector to the given 3D vector can be performed in a similar manner. Furthermore, let Θ ⊂ S be the points within an infinite cylinder centred at x c , of radius r and axis w c , given by (5) In other words, Eq. ( 5) rejects the points which have pointto-axis distances larger than the cylinder radius r, see Fig. 4 as an example. Two 3D points x 1 , x 2 are projected onto the cylinder axis wc as p 1 and p 2 . And the distances from points x 1 , x 2 to p 1 , p 2 are notated as d 1 and d 2 , respectively. Since d 1 2 = xc − x 1 2 − P(x 1 , wc) 2 ≤ r 2 , x 1 ∈ Θ is considered as inside the cylinder. In contrast, x 2 / ∈ Θ is outside the cylinder. Hereafter, we define some notations for matrix operation. Let A = (a ij ) be the element-wise representation of an m × n -sized matrix. Its column-wise representation is notated as A = [a 1 , • • • a j , . . . a n ] where a j is an m-dimensional vector. A 0 means that A is a symmetric and positive semi-definite matrix." ;
        nif:beginIndex            "18074"^^xsd:nonNegativeInteger ;
        nif:endIndex              "20730"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "II. FUNDAMENTAL DEFINITIONS AND NOTATIONS Let" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .

<http://scilake-project.eu/res/7d2c44be#offset_50_591>
        a                         <dfki:ScientificDocumentPart> , nif:OffsetBasedString ;
        nif:anchorOf              "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés." ;
        nif:beginIndex            "50"^^xsd:nonNegativeInteger ;
        nif:endIndex              "591"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés." ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "abstract" .

<http://scilake-project.eu/res/7d2c44be#offset_24945_27630>
        a                         nif:OffsetBasedString , <dfki:ScientificDocumentPart> ;
        nif:anchorOf              "Practical scenarios, in which the sizes and the speeds of objects may significantly vary (i.e. from pedestrians to trucks), impose to the scene analysis in a dynamic manner. A default size of the local bounding box may cover only a small (or respectively too large) part of the object. This problem can be effectively addressed by taking a relatively large bounding box with a radius-variance enclosing cylinder, where the cylinder radius is inversely proportional to the number of points within the enclosing cylinder. Our analysis algorithm is mostly driven by three parameters, namely the size of bounding box, its location, and the radius of the enclosing cylinder. To point out, we apply the bounding box (rather than an ellipsoid) for fast neighbourhood searching of Eq. ( 7) on the local flow field estimation. These three parameters can be reduced to two by taking a fixsized bounding box with the radius as a ratio of its size. A motion is considered as \"slow\" when the sequential point sets S are totally bounded by the pre-defined bounding box. Consequently, the slow motions are not problematic because the corresponding point sets remain in the same bounding box. Otherwise, the bounding box is translated along the motion flow direction to obtain a larger coverage. As soon as the consecutive frames have led to a coherent motion, the local neighbourhood is updated, as illustrated in Fig. 6. In this figure, the bounding box is supposed to cover 9 consecutive frames for the object's motion analysis, however, only 5 consecutive frames are covered within the given sized bounding box due to the large displacement of the moving object. In order to achieve a larger coverage, the bounding box is translated along the motion flow direction. Followed by, the enclosing cylinder is applied for the object's motion behaviour study. Regarding to the parameter reduction, it is sufficient to choose a radius that is 20% smaller than the size of the bounding box according to our experiments. Moreover, this radius is proportionally adapted to the distance between the object and the camera. Formally, we use a dynamic searching strategy along the flow direction. Let B = {B t , t = 1, . . . , f } be the assembly of f frames of point sets within a local bounding box. When a fast motion occurs, the bounding box (centred at x c ) covers f frames with f < n, where n is the objective frame length for motion analysis. Let P t (B t , w), t = 1, . . . , f be the projections of B along the motion direction w, and δ t = median(P t ), t = 1, . . . , f be the median values of projections of P t . The bounding box is translated to x t = x c + δ t w, until all n frames are covered." ;
        nif:beginIndex            "24945"^^xsd:nonNegativeInteger ;
        nif:endIndex              "27630"^^xsd:nonNegativeInteger ;
        nif:referenceContext      <http://scilake-project.eu/res/7d2c44be> ;
        dct:section_number        "" ;
        dct:source                "" ;
        dct:title                 "C. Dynamic Neighbourhood Search" ;
        qont:metadata             []  ;
        scilake:DocumentPartType  "section" .
